{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8892,
     "status": "ok",
     "timestamp": 1760403515639,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "pyVtkLH5SqDt"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Purpose: Train an MLP regression model on Revenue\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from google.colab import drive\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19506,
     "status": "ok",
     "timestamp": 1760403535151,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "-lrbuPklS3J1",
    "outputId": "0121bdc5-8234-471b-aeeb-9336cb3d7cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Setup & Configurations\n",
    "# ============================================================\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# File paths\n",
    "TRAIN_CSV_PATH = \"train.csv\"\n",
    "TEST_CSV_PATH  = \"test.csv\"\n",
    "MODEL_DIR      = \"mlp_kpi_regression_smoothl1\"\n",
    "\n",
    "# Data settings, using all data available\n",
    "HAS_HEADER = True\n",
    "\n",
    "#using only fundamentals\n",
    "EXCEL_FEATURE_START = \"C\"\n",
    "EXCEL_FEATURE_END   = \"BZ\"\n",
    "EXCEL_TARGET        = \"LO\" # This is the column for Revenue\n",
    "\n",
    "# Model training settings\n",
    "BATCH_SIZE   = 256\n",
    "EPOCHS       = 500\n",
    "SEED         = 42\n",
    "# NOTE: This delta is in ORIGINAL target units (since we invert-scale at the output).\n",
    "# Consider setting DELTA using your train-set P75(|y - mean(y)|) â‰ˆ 3.03e8 for stability.\n",
    "DELTA        = 1.0   # Huber delta (original target units)\n",
    "\n",
    "# Learning rate schedule\n",
    "BASE_LR      = 1e-3\n",
    "MIN_LR_RATIO = 0.01  # final lr = BASE_LR * MIN_LR_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1760403535165,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "PWN5yet3TjT4"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. Reproducibility\n",
    "# ============================================================\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1760403535217,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "d2nlpHbvT7n2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3. Helper Functions & Custom Classes\n",
    "# ============================================================\n",
    "\n",
    "def excel_col_to_zero_index(col: str) -> int:\n",
    "    col = col.strip().upper()\n",
    "    val = 0\n",
    "    for ch in col:\n",
    "        val = val * 26 + (ord(ch) - 64)\n",
    "    return val - 1\n",
    "\n",
    "class SuccessRate(tf.keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    Success Rate = fraction of predictions where |y_true - y_pred| < threshold\n",
    "    (Absolute error, in original target units.)\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=0.05, name=\"success_rate\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
    "        self.successes = self.add_weight(name=\"successes\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        success = tf.cast(error < self.threshold, tf.float32)\n",
    "        self.successes.assign_add(tf.reduce_sum(success))\n",
    "        self.total.assign_add(tf.cast(tf.size(success), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.successes / (self.total + 1e-6)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total.assign(0.0)\n",
    "        self.successes.assign(0.0)\n",
    "\n",
    "\n",
    "class LogLearningRate(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        opt = self.model.optimizer\n",
    "        lr = opt.learning_rate\n",
    "        if callable(lr):\n",
    "            lr = lr(opt.iterations)\n",
    "        lr = float(tf.keras.backend.get_value(lr))\n",
    "        print(f\" - learning_rate: {lr:.6e}\")\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"custom\")\n",
    "class WarmupCosine(LearningRateSchedule):\n",
    "    def __init__(self, base_lr, total_steps, warmup_steps=0, min_lr_ratio=0.01, name=None):\n",
    "        super().__init__()\n",
    "        self.base_lr = float(base_lr)\n",
    "        self.total_steps = int(total_steps)\n",
    "        self.warmup_steps = int(warmup_steps)\n",
    "        self.min_lr_ratio = float(min_lr_ratio)\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        base_lr = tf.constant(self.base_lr, tf.float32)\n",
    "        min_lr  = tf.constant(self.base_lr * self.min_lr_ratio, tf.float32)\n",
    "\n",
    "        def warmup():\n",
    "            return base_lr * (step / tf.maximum(1.0, float(self.warmup_steps)))\n",
    "\n",
    "        def cosine():\n",
    "            progress = (step - float(self.warmup_steps)) / tf.maximum(\n",
    "                1.0, float(self.total_steps - self.warmup_steps)\n",
    "            )\n",
    "            progress = tf.clip_by_value(progress, 0.0, 1.0)\n",
    "            cos_term = 0.5 * (1.0 + tf.cos(math.pi * progress))\n",
    "            return min_lr + (base_lr - min_lr) * cos_term\n",
    "\n",
    "        return tf.cond(step < self.warmup_steps, warmup, cosine)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"base_lr\": self.base_lr,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"min_lr_ratio\": self.min_lr_ratio,\n",
    "            \"name\": self.name,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1842,
     "status": "ok",
     "timestamp": 1760403537062,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "MHAl7AE8VhdY",
    "outputId": "8b412cf6-685c-4628-9e94-a2e0368bd2d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Target stats] mean=2673319936.000000, std=8338769920.000000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Data Loading & Preprocessing - with optional one-hot and column alignment\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def load_xy_from_csv(path, has_header=True):\n",
    "    if has_header:\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "\n",
    "    feat_start = excel_col_to_zero_index(EXCEL_FEATURE_START)\n",
    "    feat_end_inclusive = excel_col_to_zero_index(EXCEL_FEATURE_END)\n",
    "    target_idx = excel_col_to_zero_index(EXCEL_TARGET)\n",
    "\n",
    "    X_df = df.iloc[:, feat_start:feat_end_inclusive + 1].copy()\n",
    "    y_series = df.iloc[:, target_idx].copy()\n",
    "\n",
    "    # Impute features (median for numeric, mode for categorical) -- missing values already handled upstream\n",
    "    # for col in X_df.columns:\n",
    "    #     if pd.api.types.is_numeric_dtype(X_df[col]):\n",
    "    #         X_df[col] = X_df[col].fillna(X_df[col].median())\n",
    "    #     else:\n",
    "    #         mode_val = X_df[col].mode().iloc[0] if not X_df[col].mode().empty else \"\"\n",
    "    #         X_df[col] = X_df[col].fillna(mode_val)\n",
    "\n",
    "    # One-hot encode any non-numeric columns\n",
    "    non_numeric_cols = [c for c in X_df.columns if not pd.api.types.is_numeric_dtype(X_df[c])]\n",
    "    if non_numeric_cols:\n",
    "        X_df = pd.get_dummies(X_df, columns=non_numeric_cols, dummy_na=False)\n",
    "\n",
    "    # Target numeric + impute\n",
    "    #y = pd.to_numeric(y_series, errors=\"coerce\")\n",
    "    #y = y.fillna(y.median())\n",
    "    y = y_series.astype(np.float32).values.reshape(-1, 1)\n",
    "\n",
    "    return X_df, y\n",
    "\n",
    "# Load train/test\n",
    "X_train_df, y_train = load_xy_from_csv(TRAIN_CSV_PATH, HAS_HEADER)\n",
    "X_test_df,  y_test  = load_xy_from_csv(TEST_CSV_PATH,  HAS_HEADER)\n",
    "\n",
    "# Align test columns to train columns if one-hot created different sets (fill missing with 0)\n",
    "if list(X_train_df.columns) != list(X_test_df.columns):\n",
    "    X_test_df = X_test_df.reindex(columns=X_train_df.columns, fill_value=0)\n",
    "\n",
    "X_train = X_train_df.values.astype(np.float32)\n",
    "X_test  = X_test_df.values.astype(np.float32)\n",
    "\n",
    "# --------------------\n",
    "# Target stats (for stable training but original-unit metrics)\n",
    "# --------------------\n",
    "y_mean = float(np.mean(y_train))\n",
    "y_std  = float(np.std(y_train) + 1e-6)\n",
    "print(f\"[Target stats] mean={y_mean:.6f}, std={y_std:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1760403537164,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "mxxNo0CNVwDs"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Datasets & Normalization\n",
    "# ============================================================\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(X_train)\n",
    "\n",
    "steps_per_epoch = int(math.ceil(len(X_train) / BATCH_SIZE))\n",
    "total_epochs    = EPOCHS\n",
    "warmup_epochs   = 0.005 * EPOCHS  # 0.5% warmup\n",
    "\n",
    "lr_schedule = WarmupCosine(\n",
    "    base_lr=BASE_LR,\n",
    "    total_steps=steps_per_epoch * total_epochs,\n",
    "    warmup_steps=int(steps_per_epoch * warmup_epochs),\n",
    "    min_lr_ratio=MIN_LR_RATIO\n",
    ")\n",
    "\n",
    "def make_ds(X_np, y_np, training):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_np, y_np))\n",
    "    if training:\n",
    "        ds = ds.shuffle(min(len(X_np), 10000), seed=SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH_SIZE).map(lambda x, y: (normalizer(x), y)).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_ds(X_train, y_train, training=True)\n",
    "test_ds  = make_ds(X_test,  y_test,  training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1760403537301,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "5HHs3aSC-H7M"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Model Architecture - simple MLP, residuals, no LayerNorm\n",
    "# ============================================================\n",
    "\n",
    "def build_mlp(input_dim, y_mean, y_std,\n",
    "              widths=(256, 256, 128, 64, 32),\n",
    "              drop=0.05,\n",
    "              activation=\"gelu\",\n",
    "              weight_decay=1e-5):\n",
    "\n",
    "    act = (lambda x: tf.keras.activations.gelu(x, approximate=True)) if activation == \"gelu\" else activation\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(input_dim,), name=\"features\")\n",
    "    x = inputs\n",
    "    for i, w in enumerate(widths):\n",
    "        x = tf.keras.layers.Dense(\n",
    "            w,\n",
    "            activation=act,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "            name=f\"dense{i+1}\"\n",
    "        )(x)\n",
    "        if drop and drop > 0:\n",
    "            x = tf.keras.layers.Dropout(drop, name=f\"drop{i+1}\")(x)\n",
    "\n",
    "    z = tf.keras.layers.Dense(1, activation=None, name=\"z_scaled\")(x)\n",
    "    y_hat = tf.keras.layers.Lambda(lambda t: t * y_std + y_mean, name=\"y_hat\")(z)\n",
    "\n",
    "    model = tf.keras.Model(inputs, y_hat, name=\"mlp_simple_tuned\")\n",
    "\n",
    "    huber = tf.keras.losses.Huber(delta=DELTA)\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule,\n",
    "        weight_decay=weight_decay,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=huber,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
    "            tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n",
    "            SuccessRate(threshold=0.05, name=\"success_rate\"),\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_mlp(\n",
    "    input_dim=X_train.shape[1],\n",
    "    y_mean=y_mean, y_std=y_std,\n",
    "    widths=(256, 256, 128, 64, 32),\n",
    "    drop=0.05,\n",
    "    activation=\"gelu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1760403537320,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "Q_GRllYuWaBg"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. Callbacks\n",
    "# ============================================================\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(MODEL_DIR, \"best_model.keras\"),\n",
    "        monitor=\"loss\",\n",
    "        mode=\"min\",\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    LogLearningRate(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73014,
     "status": "ok",
     "timestamp": 1760403610336,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "7Yzijp3SWbaW",
    "outputId": "1af3562d-07e9-43aa-fdcd-5c53578a3ff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - learning_rate: 4.000000e-04\n",
      "6/6 - 5s - 903ms/step - loss: 3164151296.0000 - mae: 3164151296.0000 - rmse: 8432394240.0000 - success_rate: 0.0000e+00\n",
      "Epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/activations/__init__.py:78: UserWarning: The object being serialized includes a `lambda`. This is unsafe. In order to reload the object, you will have to pass `safe_mode=False` to the loading function. Please avoid using `lambda` in the future, and use named Python functions instead. This is the `lambda` being serialized:     act = (lambda x: tf.keras.activations.gelu(x, approximate=True)) if activation == \"gelu\" else activation\n",
      "\n",
      "  fn_config = serialization_lib.serialize_keras_object(activation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - learning_rate: 8.000000e-04\n",
      "6/6 - 0s - 45ms/step - loss: 2001186176.0000 - mae: 2001186176.0000 - rmse: 6667919872.0000 - success_rate: 0.0000e+00\n",
      "Epoch 3/500\n",
      " - learning_rate: 9.999976e-04\n",
      "6/6 - 0s - 49ms/step - loss: 1624415488.0000 - mae: 1624415488.0000 - rmse: 4505021440.0000 - success_rate: 0.0000e+00\n",
      "Epoch 4/500\n",
      " - learning_rate: 9.999778e-04\n",
      "6/6 - 0s - 40ms/step - loss: 959558848.0000 - mae: 959558848.0000 - rmse: 2358845440.0000 - success_rate: 0.0000e+00\n",
      "Epoch 5/500\n",
      " - learning_rate: 9.999383e-04\n",
      "6/6 - 0s - 48ms/step - loss: 864643840.0000 - mae: 864643840.0000 - rmse: 2569158144.0000 - success_rate: 0.0000e+00\n",
      "Epoch 6/500\n",
      " - learning_rate: 9.998791e-04\n",
      "6/6 - 0s - 28ms/step - loss: 700395072.0000 - mae: 700395072.0000 - rmse: 2154988032.0000 - success_rate: 0.0000e+00\n",
      "Epoch 7/500\n",
      " - learning_rate: 9.998003e-04\n",
      "6/6 - 0s - 28ms/step - loss: 608420672.0000 - mae: 608420672.0000 - rmse: 1915117056.0000 - success_rate: 0.0000e+00\n",
      "Epoch 8/500\n",
      " - learning_rate: 9.997016e-04\n",
      "6/6 - 0s - 30ms/step - loss: 562528640.0000 - mae: 562528640.0000 - rmse: 1714744960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 9/500\n",
      " - learning_rate: 9.995830e-04\n",
      "6/6 - 0s - 16ms/step - loss: 657713856.0000 - mae: 657713856.0000 - rmse: 3080496128.0000 - success_rate: 0.0000e+00\n",
      "Epoch 10/500\n",
      " - learning_rate: 9.994450e-04\n",
      "6/6 - 0s - 33ms/step - loss: 506843168.0000 - mae: 506843168.0000 - rmse: 1481726080.0000 - success_rate: 0.0000e+00\n",
      "Epoch 11/500\n",
      " - learning_rate: 9.992871e-04\n",
      "6/6 - 0s - 27ms/step - loss: 480502976.0000 - mae: 480502976.0000 - rmse: 1362981120.0000 - success_rate: 0.0000e+00\n",
      "Epoch 12/500\n",
      " - learning_rate: 9.991096e-04\n",
      "6/6 - 0s - 17ms/step - loss: 528683168.0000 - mae: 528683168.0000 - rmse: 2294738176.0000 - success_rate: 0.0000e+00\n",
      "Epoch 13/500\n",
      " - learning_rate: 9.989124e-04\n",
      "6/6 - 0s - 17ms/step - loss: 587971776.0000 - mae: 587971776.0000 - rmse: 2781043712.0000 - success_rate: 0.0000e+00\n",
      "Epoch 14/500\n",
      " - learning_rate: 9.986954e-04\n",
      "6/6 - 0s - 16ms/step - loss: 557299264.0000 - mae: 557299264.0000 - rmse: 1929856896.0000 - success_rate: 0.0000e+00\n",
      "Epoch 15/500\n",
      " - learning_rate: 9.984587e-04\n",
      "6/6 - 0s - 27ms/step - loss: 460473984.0000 - mae: 460473984.0000 - rmse: 1304696832.0000 - success_rate: 0.0000e+00\n",
      "Epoch 16/500\n",
      " - learning_rate: 9.982024e-04\n",
      "6/6 - 0s - 19ms/step - loss: 587332992.0000 - mae: 587332992.0000 - rmse: 2798440960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 17/500\n",
      " - learning_rate: 9.979265e-04\n",
      "6/6 - 0s - 16ms/step - loss: 462470560.0000 - mae: 462470560.0000 - rmse: 1349813504.0000 - success_rate: 0.0000e+00\n",
      "Epoch 18/500\n",
      " - learning_rate: 9.976309e-04\n",
      "6/6 - 0s - 16ms/step - loss: 469188704.0000 - mae: 469188704.0000 - rmse: 1740304768.0000 - success_rate: 0.0000e+00\n",
      "Epoch 19/500\n",
      " - learning_rate: 9.973155e-04\n",
      "6/6 - 0s - 30ms/step - loss: 450986336.0000 - mae: 450986336.0000 - rmse: 1613563776.0000 - success_rate: 0.0000e+00\n",
      "Epoch 20/500\n",
      " - learning_rate: 9.969806e-04\n",
      "6/6 - 0s - 18ms/step - loss: 463105184.0000 - mae: 463105184.0000 - rmse: 1603471616.0000 - success_rate: 0.0000e+00\n",
      "Epoch 21/500\n",
      " - learning_rate: 9.966261e-04\n",
      "6/6 - 0s - 28ms/step - loss: 428204576.0000 - mae: 428204640.0000 - rmse: 1509802112.0000 - success_rate: 0.0000e+00\n",
      "Epoch 22/500\n",
      " - learning_rate: 9.962519e-04\n",
      "6/6 - 0s - 16ms/step - loss: 456412736.0000 - mae: 456412736.0000 - rmse: 1806040704.0000 - success_rate: 0.0000e+00\n",
      "Epoch 23/500\n",
      " - learning_rate: 9.958582e-04\n",
      "6/6 - 0s - 29ms/step - loss: 402521728.0000 - mae: 402521728.0000 - rmse: 1345282816.0000 - success_rate: 0.0000e+00\n",
      "Epoch 24/500\n",
      " - learning_rate: 9.954449e-04\n",
      "6/6 - 0s - 16ms/step - loss: 418193184.0000 - mae: 418193184.0000 - rmse: 1332531968.0000 - success_rate: 0.0000e+00\n",
      "Epoch 25/500\n",
      " - learning_rate: 9.950120e-04\n",
      "6/6 - 0s - 17ms/step - loss: 409114848.0000 - mae: 409114848.0000 - rmse: 1450545536.0000 - success_rate: 0.0000e+00\n",
      "Epoch 26/500\n",
      " - learning_rate: 9.945597e-04\n",
      "6/6 - 0s - 16ms/step - loss: 428623904.0000 - mae: 428623904.0000 - rmse: 1536861952.0000 - success_rate: 0.0000e+00\n",
      "Epoch 27/500\n",
      " - learning_rate: 9.940878e-04\n",
      "6/6 - 0s - 41ms/step - loss: 394689696.0000 - mae: 394689696.0000 - rmse: 1439166848.0000 - success_rate: 0.0000e+00\n",
      "Epoch 28/500\n",
      " - learning_rate: 9.935963e-04\n",
      "6/6 - 0s - 25ms/step - loss: 450368672.0000 - mae: 450368672.0000 - rmse: 1605891200.0000 - success_rate: 0.0000e+00\n",
      "Epoch 29/500\n",
      " - learning_rate: 9.930854e-04\n",
      "6/6 - 0s - 52ms/step - loss: 400777088.0000 - mae: 400777088.0000 - rmse: 1364620160.0000 - success_rate: 0.0000e+00\n",
      "Epoch 30/500\n",
      " - learning_rate: 9.925551e-04\n",
      "6/6 - 0s - 49ms/step - loss: 395409632.0000 - mae: 395409632.0000 - rmse: 1266779520.0000 - success_rate: 0.0000e+00\n",
      "Epoch 31/500\n",
      " - learning_rate: 9.920052e-04\n",
      "6/6 - 0s - 50ms/step - loss: 407601568.0000 - mae: 407601568.0000 - rmse: 1882474496.0000 - success_rate: 0.0000e+00\n",
      "Epoch 32/500\n",
      " - learning_rate: 9.914360e-04\n",
      "6/6 - 0s - 27ms/step - loss: 520690624.0000 - mae: 520690624.0000 - rmse: 2038564352.0000 - success_rate: 0.0000e+00\n",
      "Epoch 33/500\n",
      " - learning_rate: 9.908475e-04\n",
      "6/6 - 0s - 24ms/step - loss: 458707872.0000 - mae: 458707872.0000 - rmse: 1816296704.0000 - success_rate: 0.0000e+00\n",
      "Epoch 34/500\n",
      " - learning_rate: 9.902394e-04\n",
      "6/6 - 0s - 51ms/step - loss: 401677888.0000 - mae: 401677888.0000 - rmse: 1458518528.0000 - success_rate: 0.0000e+00\n",
      "Epoch 35/500\n",
      " - learning_rate: 9.896121e-04\n",
      "6/6 - 0s - 38ms/step - loss: 374686592.0000 - mae: 374686592.0000 - rmse: 1154178688.0000 - success_rate: 0.0000e+00\n",
      "Epoch 36/500\n",
      " - learning_rate: 9.889654e-04\n",
      "6/6 - 0s - 38ms/step - loss: 395380736.0000 - mae: 395380736.0000 - rmse: 1544076160.0000 - success_rate: 0.0000e+00\n",
      "Epoch 37/500\n",
      " - learning_rate: 9.882995e-04\n",
      "6/6 - 0s - 65ms/step - loss: 358902848.0000 - mae: 358902848.0000 - rmse: 1220015616.0000 - success_rate: 0.0000e+00\n",
      "Epoch 38/500\n",
      " - learning_rate: 9.876142e-04\n",
      "6/6 - 0s - 34ms/step - loss: 409882304.0000 - mae: 409882304.0000 - rmse: 1366956032.0000 - success_rate: 0.0000e+00\n",
      "Epoch 39/500\n",
      " - learning_rate: 9.869097e-04\n",
      "6/6 - 0s - 27ms/step - loss: 389705120.0000 - mae: 389705120.0000 - rmse: 1297132288.0000 - success_rate: 0.0000e+00\n",
      "Epoch 40/500\n",
      " - learning_rate: 9.861861e-04\n",
      "6/6 - 0s - 40ms/step - loss: 329606752.0000 - mae: 329606752.0000 - rmse: 1057269760.0000 - success_rate: 0.0000e+00\n",
      "Epoch 41/500\n",
      " - learning_rate: 9.854431e-04\n",
      "6/6 - 0s - 27ms/step - loss: 360324064.0000 - mae: 360324064.0000 - rmse: 1099415808.0000 - success_rate: 0.0000e+00\n",
      "Epoch 42/500\n",
      " - learning_rate: 9.846811e-04\n",
      "6/6 - 0s - 23ms/step - loss: 369276576.0000 - mae: 369276576.0000 - rmse: 1314898816.0000 - success_rate: 0.0000e+00\n",
      "Epoch 43/500\n",
      " - learning_rate: 9.838999e-04\n",
      "6/6 - 0s - 26ms/step - loss: 402827712.0000 - mae: 402827712.0000 - rmse: 1543755648.0000 - success_rate: 0.0000e+00\n",
      "Epoch 44/500\n",
      " - learning_rate: 9.830997e-04\n",
      "6/6 - 0s - 16ms/step - loss: 372344352.0000 - mae: 372344352.0000 - rmse: 1394108288.0000 - success_rate: 0.0000e+00\n",
      "Epoch 45/500\n",
      " - learning_rate: 9.822802e-04\n",
      "6/6 - 0s - 17ms/step - loss: 375997952.0000 - mae: 375997952.0000 - rmse: 1406349184.0000 - success_rate: 0.0000e+00\n",
      "Epoch 46/500\n",
      " - learning_rate: 9.814419e-04\n",
      "6/6 - 0s - 17ms/step - loss: 347070528.0000 - mae: 347070528.0000 - rmse: 1143590784.0000 - success_rate: 0.0000e+00\n",
      "Epoch 47/500\n",
      " - learning_rate: 9.805846e-04\n",
      "6/6 - 0s - 16ms/step - loss: 364915264.0000 - mae: 364915264.0000 - rmse: 1320518656.0000 - success_rate: 0.0000e+00\n",
      "Epoch 48/500\n",
      " - learning_rate: 9.797082e-04\n",
      "6/6 - 0s - 26ms/step - loss: 321962912.0000 - mae: 321962912.0000 - rmse: 887543360.0000 - success_rate: 0.0000e+00\n",
      "Epoch 49/500\n",
      " - learning_rate: 9.788129e-04\n",
      "6/6 - 0s - 16ms/step - loss: 346181600.0000 - mae: 346181600.0000 - rmse: 1113783552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 50/500\n",
      " - learning_rate: 9.778987e-04\n",
      "6/6 - 0s - 15ms/step - loss: 371392160.0000 - mae: 371392160.0000 - rmse: 1341744256.0000 - success_rate: 0.0000e+00\n",
      "Epoch 51/500\n",
      " - learning_rate: 9.769658e-04\n",
      "6/6 - 0s - 15ms/step - loss: 347497728.0000 - mae: 347497728.0000 - rmse: 1016551296.0000 - success_rate: 0.0000e+00\n",
      "Epoch 52/500\n",
      " - learning_rate: 9.760140e-04\n",
      "6/6 - 0s - 17ms/step - loss: 338872096.0000 - mae: 338872096.0000 - rmse: 1014648512.0000 - success_rate: 0.0000e+00\n",
      "Epoch 53/500\n",
      " - learning_rate: 9.750433e-04\n",
      "6/6 - 0s - 17ms/step - loss: 342509760.0000 - mae: 342509760.0000 - rmse: 1114164864.0000 - success_rate: 0.0000e+00\n",
      "Epoch 54/500\n",
      " - learning_rate: 9.740540e-04\n",
      "6/6 - 0s - 15ms/step - loss: 377826368.0000 - mae: 377826368.0000 - rmse: 1405922816.0000 - success_rate: 0.0000e+00\n",
      "Epoch 55/500\n",
      " - learning_rate: 9.730459e-04\n",
      "6/6 - 0s - 24ms/step - loss: 362372448.0000 - mae: 362372448.0000 - rmse: 1135786240.0000 - success_rate: 0.0000e+00\n",
      "Epoch 56/500\n",
      " - learning_rate: 9.720192e-04\n",
      "6/6 - 0s - 16ms/step - loss: 335723872.0000 - mae: 335723872.0000 - rmse: 908775680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 57/500\n",
      " - learning_rate: 9.709738e-04\n",
      "6/6 - 0s - 16ms/step - loss: 375716640.0000 - mae: 375716640.0000 - rmse: 1428806144.0000 - success_rate: 0.0000e+00\n",
      "Epoch 58/500\n",
      " - learning_rate: 9.699099e-04\n",
      "6/6 - 0s - 15ms/step - loss: 358633984.0000 - mae: 358633984.0000 - rmse: 1433560448.0000 - success_rate: 0.0000e+00\n",
      "Epoch 59/500\n",
      " - learning_rate: 9.688275e-04\n",
      "6/6 - 0s - 16ms/step - loss: 392578144.0000 - mae: 392578144.0000 - rmse: 1292016640.0000 - success_rate: 0.0000e+00\n",
      "Epoch 60/500\n",
      " - learning_rate: 9.677265e-04\n",
      "6/6 - 0s - 22ms/step - loss: 366129952.0000 - mae: 366129952.0000 - rmse: 1280743040.0000 - success_rate: 0.0000e+00\n",
      "Epoch 61/500\n",
      " - learning_rate: 9.666070e-04\n",
      "6/6 - 0s - 16ms/step - loss: 384590688.0000 - mae: 384590688.0000 - rmse: 1177309056.0000 - success_rate: 0.0000e+00\n",
      "Epoch 62/500\n",
      " - learning_rate: 9.654692e-04\n",
      "6/6 - 0s - 19ms/step - loss: 347902112.0000 - mae: 347902112.0000 - rmse: 927901248.0000 - success_rate: 0.0000e+00\n",
      "Epoch 63/500\n",
      " - learning_rate: 9.643130e-04\n",
      "6/6 - 0s - 19ms/step - loss: 378300352.0000 - mae: 378300352.0000 - rmse: 1199003904.0000 - success_rate: 0.0000e+00\n",
      "Epoch 64/500\n",
      " - learning_rate: 9.631385e-04\n",
      "6/6 - 0s - 18ms/step - loss: 376789856.0000 - mae: 376789856.0000 - rmse: 1250578944.0000 - success_rate: 0.0000e+00\n",
      "Epoch 65/500\n",
      " - learning_rate: 9.619458e-04\n",
      "6/6 - 0s - 16ms/step - loss: 377398176.0000 - mae: 377398176.0000 - rmse: 1341500416.0000 - success_rate: 0.0000e+00\n",
      "Epoch 66/500\n",
      " - learning_rate: 9.607348e-04\n",
      "6/6 - 0s - 16ms/step - loss: 350876672.0000 - mae: 350876672.0000 - rmse: 1073290368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 67/500\n",
      " - learning_rate: 9.595056e-04\n",
      "6/6 - 0s - 15ms/step - loss: 372537408.0000 - mae: 372537408.0000 - rmse: 1297578752.0000 - success_rate: 0.0000e+00\n",
      "Epoch 68/500\n",
      " - learning_rate: 9.582583e-04\n",
      "6/6 - 0s - 33ms/step - loss: 303289440.0000 - mae: 303289440.0000 - rmse: 806674496.0000 - success_rate: 0.0000e+00\n",
      "Epoch 69/500\n",
      " - learning_rate: 9.569930e-04\n",
      "6/6 - 0s - 16ms/step - loss: 344098464.0000 - mae: 344098464.0000 - rmse: 1528072576.0000 - success_rate: 0.0000e+00\n",
      "Epoch 70/500\n",
      " - learning_rate: 9.557095e-04\n",
      "6/6 - 0s - 25ms/step - loss: 347186496.0000 - mae: 347186496.0000 - rmse: 1496676224.0000 - success_rate: 0.0000e+00\n",
      "Epoch 71/500\n",
      " - learning_rate: 9.544082e-04\n",
      "6/6 - 0s - 16ms/step - loss: 384517504.0000 - mae: 384517504.0000 - rmse: 1582049024.0000 - success_rate: 0.0000e+00\n",
      "Epoch 72/500\n",
      " - learning_rate: 9.530889e-04\n",
      "6/6 - 0s - 23ms/step - loss: 417668224.0000 - mae: 417668224.0000 - rmse: 2304511232.0000 - success_rate: 0.0000e+00\n",
      "Epoch 73/500\n",
      " - learning_rate: 9.517518e-04\n",
      "6/6 - 0s - 16ms/step - loss: 362959424.0000 - mae: 362959424.0000 - rmse: 1071166336.0000 - success_rate: 0.0000e+00\n",
      "Epoch 74/500\n",
      " - learning_rate: 9.503968e-04\n",
      "6/6 - 0s - 16ms/step - loss: 360834752.0000 - mae: 360834752.0000 - rmse: 1456592256.0000 - success_rate: 0.0000e+00\n",
      "Epoch 75/500\n",
      " - learning_rate: 9.490241e-04\n",
      "6/6 - 0s - 15ms/step - loss: 491027552.0000 - mae: 491027552.0000 - rmse: 2438051328.0000 - success_rate: 0.0000e+00\n",
      "Epoch 76/500\n",
      " - learning_rate: 9.476336e-04\n",
      "6/6 - 0s - 16ms/step - loss: 395108928.0000 - mae: 395108928.0000 - rmse: 1653992576.0000 - success_rate: 0.0000e+00\n",
      "Epoch 77/500\n",
      " - learning_rate: 9.462255e-04\n",
      "6/6 - 0s - 22ms/step - loss: 400810112.0000 - mae: 400810112.0000 - rmse: 1706334080.0000 - success_rate: 0.0000e+00\n",
      "Epoch 78/500\n",
      " - learning_rate: 9.447999e-04\n",
      "6/6 - 0s - 23ms/step - loss: 359835008.0000 - mae: 359835008.0000 - rmse: 1347578752.0000 - success_rate: 0.0000e+00\n",
      "Epoch 79/500\n",
      " - learning_rate: 9.433567e-04\n",
      "6/6 - 0s - 26ms/step - loss: 336684672.0000 - mae: 336684672.0000 - rmse: 1230140672.0000 - success_rate: 0.0000e+00\n",
      "Epoch 80/500\n",
      " - learning_rate: 9.418960e-04\n",
      "6/6 - 0s - 16ms/step - loss: 355763808.0000 - mae: 355763808.0000 - rmse: 1249875456.0000 - success_rate: 0.0000e+00\n",
      "Epoch 81/500\n",
      " - learning_rate: 9.404179e-04\n",
      "6/6 - 0s - 16ms/step - loss: 326723232.0000 - mae: 326723232.0000 - rmse: 1017035648.0000 - success_rate: 0.0000e+00\n",
      "Epoch 82/500\n",
      " - learning_rate: 9.389224e-04\n",
      "6/6 - 0s - 15ms/step - loss: 336551264.0000 - mae: 336551264.0000 - rmse: 1082712576.0000 - success_rate: 0.0000e+00\n",
      "Epoch 83/500\n",
      " - learning_rate: 9.374096e-04\n",
      "6/6 - 0s - 23ms/step - loss: 361347264.0000 - mae: 361347264.0000 - rmse: 1393933184.0000 - success_rate: 0.0000e+00\n",
      "Epoch 84/500\n",
      " - learning_rate: 9.358796e-04\n",
      "6/6 - 0s - 16ms/step - loss: 330739936.0000 - mae: 330739936.0000 - rmse: 1095536128.0000 - success_rate: 0.0000e+00\n",
      "Epoch 85/500\n",
      " - learning_rate: 9.343324e-04\n",
      "6/6 - 0s - 16ms/step - loss: 376136640.0000 - mae: 376136640.0000 - rmse: 1388470400.0000 - success_rate: 0.0000e+00\n",
      "Epoch 86/500\n",
      " - learning_rate: 9.327680e-04\n",
      "6/6 - 0s - 33ms/step - loss: 300932256.0000 - mae: 300932256.0000 - rmse: 1047347456.0000 - success_rate: 0.0000e+00\n",
      "Epoch 87/500\n",
      " - learning_rate: 9.311867e-04\n",
      "6/6 - 0s - 19ms/step - loss: 309425440.0000 - mae: 309425440.0000 - rmse: 943093312.0000 - success_rate: 0.0000e+00\n",
      "Epoch 88/500\n",
      " - learning_rate: 9.295883e-04\n",
      "6/6 - 0s - 16ms/step - loss: 321629632.0000 - mae: 321629632.0000 - rmse: 1032378240.0000 - success_rate: 0.0000e+00\n",
      "Epoch 89/500\n",
      " - learning_rate: 9.279730e-04\n",
      "6/6 - 0s - 16ms/step - loss: 320820160.0000 - mae: 320820160.0000 - rmse: 1207092480.0000 - success_rate: 0.0000e+00\n",
      "Epoch 90/500\n",
      " - learning_rate: 9.263409e-04\n",
      "6/6 - 0s - 22ms/step - loss: 387702848.0000 - mae: 387702848.0000 - rmse: 1769875840.0000 - success_rate: 0.0000e+00\n",
      "Epoch 91/500\n",
      " - learning_rate: 9.246918e-04\n",
      "6/6 - 0s - 27ms/step - loss: 297856352.0000 - mae: 297856352.0000 - rmse: 1009326592.0000 - success_rate: 0.0000e+00\n",
      "Epoch 92/500\n",
      " - learning_rate: 9.230262e-04\n",
      "6/6 - 0s - 16ms/step - loss: 336278528.0000 - mae: 336278528.0000 - rmse: 1784222720.0000 - success_rate: 0.0000e+00\n",
      "Epoch 93/500\n",
      " - learning_rate: 9.213437e-04\n",
      "6/6 - 0s - 23ms/step - loss: 320519360.0000 - mae: 320519360.0000 - rmse: 1145436288.0000 - success_rate: 0.0000e+00\n",
      "Epoch 94/500\n",
      " - learning_rate: 9.196448e-04\n",
      "6/6 - 0s - 16ms/step - loss: 335557632.0000 - mae: 335557632.0000 - rmse: 1244028160.0000 - success_rate: 0.0000e+00\n",
      "Epoch 95/500\n",
      " - learning_rate: 9.179293e-04\n",
      "6/6 - 0s - 16ms/step - loss: 322754112.0000 - mae: 322754112.0000 - rmse: 1392609408.0000 - success_rate: 0.0000e+00\n",
      "Epoch 96/500\n",
      " - learning_rate: 9.161973e-04\n",
      "6/6 - 0s - 20ms/step - loss: 324845568.0000 - mae: 324845568.0000 - rmse: 1277267328.0000 - success_rate: 0.0000e+00\n",
      "Epoch 97/500\n",
      " - learning_rate: 9.144489e-04\n",
      "6/6 - 0s - 16ms/step - loss: 359082656.0000 - mae: 359082656.0000 - rmse: 1335720704.0000 - success_rate: 0.0000e+00\n",
      "Epoch 98/500\n",
      " - learning_rate: 9.126842e-04\n",
      "6/6 - 0s - 22ms/step - loss: 343844128.0000 - mae: 343844128.0000 - rmse: 1380045568.0000 - success_rate: 0.0000e+00\n",
      "Epoch 99/500\n",
      " - learning_rate: 9.109033e-04\n",
      "6/6 - 0s - 16ms/step - loss: 366191776.0000 - mae: 366191808.0000 - rmse: 1209103744.0000 - success_rate: 0.0000e+00\n",
      "Epoch 100/500\n",
      " - learning_rate: 9.091061e-04\n",
      "6/6 - 0s - 34ms/step - loss: 258108976.0000 - mae: 258108976.0000 - rmse: 628786432.0000 - success_rate: 0.0000e+00\n",
      "Epoch 101/500\n",
      " - learning_rate: 9.072928e-04\n",
      "6/6 - 0s - 17ms/step - loss: 295031424.0000 - mae: 295031424.0000 - rmse: 1010721600.0000 - success_rate: 0.0000e+00\n",
      "Epoch 102/500\n",
      " - learning_rate: 9.054635e-04\n",
      "6/6 - 0s - 16ms/step - loss: 344476640.0000 - mae: 344476640.0000 - rmse: 1536945024.0000 - success_rate: 0.0000e+00\n",
      "Epoch 103/500\n",
      " - learning_rate: 9.036182e-04\n",
      "6/6 - 0s - 17ms/step - loss: 320658880.0000 - mae: 320658880.0000 - rmse: 1025314816.0000 - success_rate: 0.0000e+00\n",
      "Epoch 104/500\n",
      " - learning_rate: 9.017570e-04\n",
      "6/6 - 0s - 19ms/step - loss: 316493216.0000 - mae: 316493216.0000 - rmse: 1074987904.0000 - success_rate: 0.0000e+00\n",
      "Epoch 105/500\n",
      " - learning_rate: 8.998801e-04\n",
      "6/6 - 0s - 16ms/step - loss: 297575200.0000 - mae: 297575200.0000 - rmse: 1161050368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 106/500\n",
      " - learning_rate: 8.979873e-04\n",
      "6/6 - 0s - 16ms/step - loss: 316214688.0000 - mae: 316214688.0000 - rmse: 1243463552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 107/500\n",
      " - learning_rate: 8.960789e-04\n",
      "6/6 - 0s - 16ms/step - loss: 361505152.0000 - mae: 361505152.0000 - rmse: 1715251072.0000 - success_rate: 0.0000e+00\n",
      "Epoch 108/500\n",
      " - learning_rate: 8.941549e-04\n",
      "6/6 - 0s - 16ms/step - loss: 311257536.0000 - mae: 311257536.0000 - rmse: 1119151872.0000 - success_rate: 0.0000e+00\n",
      "Epoch 109/500\n",
      " - learning_rate: 8.922154e-04\n",
      "6/6 - 0s - 15ms/step - loss: 323072608.0000 - mae: 323072608.0000 - rmse: 1380824576.0000 - success_rate: 0.0000e+00\n",
      "Epoch 110/500\n",
      " - learning_rate: 8.902604e-04\n",
      "6/6 - 0s - 23ms/step - loss: 297104128.0000 - mae: 297104128.0000 - rmse: 1063753728.0000 - success_rate: 0.0000e+00\n",
      "Epoch 111/500\n",
      " - learning_rate: 8.882901e-04\n",
      "6/6 - 0s - 16ms/step - loss: 295705024.0000 - mae: 295705024.0000 - rmse: 1105107200.0000 - success_rate: 0.0000e+00\n",
      "Epoch 112/500\n",
      " - learning_rate: 8.863045e-04\n",
      "6/6 - 0s - 16ms/step - loss: 281235936.0000 - mae: 281235936.0000 - rmse: 887543680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 113/500\n",
      " - learning_rate: 8.843037e-04\n",
      "6/6 - 0s - 16ms/step - loss: 315277408.0000 - mae: 315277408.0000 - rmse: 980211456.0000 - success_rate: 0.0000e+00\n",
      "Epoch 114/500\n",
      " - learning_rate: 8.822877e-04\n",
      "6/6 - 0s - 20ms/step - loss: 299032768.0000 - mae: 299032768.0000 - rmse: 1189394560.0000 - success_rate: 0.0000e+00\n",
      "Epoch 115/500\n",
      " - learning_rate: 8.802568e-04\n",
      "6/6 - 0s - 16ms/step - loss: 307407552.0000 - mae: 307407552.0000 - rmse: 1044264512.0000 - success_rate: 0.0000e+00\n",
      "Epoch 116/500\n",
      " - learning_rate: 8.782108e-04\n",
      "6/6 - 0s - 16ms/step - loss: 329549888.0000 - mae: 329549888.0000 - rmse: 1414833920.0000 - success_rate: 0.0000e+00\n",
      "Epoch 117/500\n",
      " - learning_rate: 8.761500e-04\n",
      "6/6 - 0s - 23ms/step - loss: 376972960.0000 - mae: 376972960.0000 - rmse: 1805941248.0000 - success_rate: 0.0000e+00\n",
      "Epoch 118/500\n",
      " - learning_rate: 8.740744e-04\n",
      "6/6 - 0s - 16ms/step - loss: 320455424.0000 - mae: 320455424.0000 - rmse: 1155010944.0000 - success_rate: 0.0000e+00\n",
      "Epoch 119/500\n",
      " - learning_rate: 8.719840e-04\n",
      "6/6 - 0s - 23ms/step - loss: 296615232.0000 - mae: 296615232.0000 - rmse: 830394368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 120/500\n",
      " - learning_rate: 8.698790e-04\n",
      "6/6 - 0s - 23ms/step - loss: 357279776.0000 - mae: 357279776.0000 - rmse: 1326802176.0000 - success_rate: 0.0000e+00\n",
      "Epoch 121/500\n",
      " - learning_rate: 8.677595e-04\n",
      "6/6 - 0s - 16ms/step - loss: 304536544.0000 - mae: 304536544.0000 - rmse: 836805248.0000 - success_rate: 0.0000e+00\n",
      "Epoch 122/500\n",
      " - learning_rate: 8.656255e-04\n",
      "6/6 - 0s - 17ms/step - loss: 331533920.0000 - mae: 331533920.0000 - rmse: 1322261760.0000 - success_rate: 0.0000e+00\n",
      "Epoch 123/500\n",
      " - learning_rate: 8.634771e-04\n",
      "6/6 - 0s - 19ms/step - loss: 336734624.0000 - mae: 336734624.0000 - rmse: 1437645568.0000 - success_rate: 0.0000e+00\n",
      "Epoch 124/500\n",
      " - learning_rate: 8.613144e-04\n",
      "6/6 - 0s - 28ms/step - loss: 284401984.0000 - mae: 284401984.0000 - rmse: 935285440.0000 - success_rate: 0.0000e+00\n",
      "Epoch 125/500\n",
      " - learning_rate: 8.591376e-04\n",
      "6/6 - 0s - 47ms/step - loss: 334785024.0000 - mae: 334785024.0000 - rmse: 1234815616.0000 - success_rate: 0.0000e+00\n",
      "Epoch 126/500\n",
      " - learning_rate: 8.569466e-04\n",
      "6/6 - 0s - 24ms/step - loss: 290426496.0000 - mae: 290426496.0000 - rmse: 918913344.0000 - success_rate: 0.0000e+00\n",
      "Epoch 127/500\n",
      " - learning_rate: 8.547415e-04\n",
      "6/6 - 0s - 25ms/step - loss: 316702528.0000 - mae: 316702528.0000 - rmse: 1211231488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 128/500\n",
      " - learning_rate: 8.525226e-04\n",
      "6/6 - 0s - 27ms/step - loss: 367077728.0000 - mae: 367077728.0000 - rmse: 1709790080.0000 - success_rate: 0.0000e+00\n",
      "Epoch 129/500\n",
      " - learning_rate: 8.502897e-04\n",
      "6/6 - 0s - 48ms/step - loss: 300531936.0000 - mae: 300531936.0000 - rmse: 916584448.0000 - success_rate: 0.0000e+00\n",
      "Epoch 130/500\n",
      " - learning_rate: 8.480431e-04\n",
      "6/6 - 0s - 26ms/step - loss: 375516160.0000 - mae: 375516160.0000 - rmse: 1588975616.0000 - success_rate: 0.0000e+00\n",
      "Epoch 131/500\n",
      " - learning_rate: 8.457828e-04\n",
      "6/6 - 0s - 26ms/step - loss: 388186112.0000 - mae: 388186112.0000 - rmse: 1165556608.0000 - success_rate: 0.0000e+00\n",
      "Epoch 132/500\n",
      " - learning_rate: 8.435089e-04\n",
      "6/6 - 0s - 26ms/step - loss: 318635008.0000 - mae: 318635008.0000 - rmse: 1226810112.0000 - success_rate: 0.0000e+00\n",
      "Epoch 133/500\n",
      " - learning_rate: 8.412216e-04\n",
      "6/6 - 0s - 26ms/step - loss: 276003264.0000 - mae: 276003264.0000 - rmse: 784193408.0000 - success_rate: 0.0000e+00\n",
      "Epoch 134/500\n",
      " - learning_rate: 8.389208e-04\n",
      "6/6 - 0s - 29ms/step - loss: 282267936.0000 - mae: 282267936.0000 - rmse: 1036359040.0000 - success_rate: 0.0000e+00\n",
      "Epoch 135/500\n",
      " - learning_rate: 8.366067e-04\n",
      "6/6 - 0s - 46ms/step - loss: 314354304.0000 - mae: 314354304.0000 - rmse: 1277572480.0000 - success_rate: 0.0000e+00\n",
      "Epoch 136/500\n",
      " - learning_rate: 8.342794e-04\n",
      "6/6 - 0s - 29ms/step - loss: 278585632.0000 - mae: 278585632.0000 - rmse: 1131139072.0000 - success_rate: 0.0000e+00\n",
      "Epoch 137/500\n",
      " - learning_rate: 8.319389e-04\n",
      "6/6 - 0s - 26ms/step - loss: 259377696.0000 - mae: 259377696.0000 - rmse: 781502912.0000 - success_rate: 0.0000e+00\n",
      "Epoch 138/500\n",
      " - learning_rate: 8.295854e-04\n",
      "6/6 - 0s - 49ms/step - loss: 293303936.0000 - mae: 293303936.0000 - rmse: 851740992.0000 - success_rate: 0.0000e+00\n",
      "Epoch 139/500\n",
      " - learning_rate: 8.272190e-04\n",
      "6/6 - 0s - 52ms/step - loss: 273521408.0000 - mae: 273521408.0000 - rmse: 798186368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 140/500\n",
      " - learning_rate: 8.248397e-04\n",
      "6/6 - 0s - 42ms/step - loss: 253451984.0000 - mae: 253451984.0000 - rmse: 746427968.0000 - success_rate: 0.0000e+00\n",
      "Epoch 141/500\n",
      " - learning_rate: 8.224477e-04\n",
      "6/6 - 0s - 24ms/step - loss: 293238720.0000 - mae: 293238720.0000 - rmse: 996414848.0000 - success_rate: 0.0000e+00\n",
      "Epoch 142/500\n",
      " - learning_rate: 8.200430e-04\n",
      "6/6 - 0s - 24ms/step - loss: 272736928.0000 - mae: 272736960.0000 - rmse: 918944320.0000 - success_rate: 0.0000e+00\n",
      "Epoch 143/500\n",
      " - learning_rate: 8.176258e-04\n",
      "6/6 - 0s - 17ms/step - loss: 289023712.0000 - mae: 289023712.0000 - rmse: 1018358592.0000 - success_rate: 0.0000e+00\n",
      "Epoch 144/500\n",
      " - learning_rate: 8.151960e-04\n",
      "6/6 - 0s - 16ms/step - loss: 291637280.0000 - mae: 291637280.0000 - rmse: 1122787456.0000 - success_rate: 0.0000e+00\n",
      "Epoch 145/500\n",
      " - learning_rate: 8.127539e-04\n",
      "6/6 - 0s - 20ms/step - loss: 271210912.0000 - mae: 271210912.0000 - rmse: 882110400.0000 - success_rate: 0.0000e+00\n",
      "Epoch 146/500\n",
      " - learning_rate: 8.102996e-04\n",
      "6/6 - 0s - 16ms/step - loss: 273914528.0000 - mae: 273914528.0000 - rmse: 1242441856.0000 - success_rate: 0.0000e+00\n",
      "Epoch 147/500\n",
      " - learning_rate: 8.078330e-04\n",
      "6/6 - 0s - 16ms/step - loss: 289492544.0000 - mae: 289492544.0000 - rmse: 1336265472.0000 - success_rate: 0.0000e+00\n",
      "Epoch 148/500\n",
      " - learning_rate: 8.053544e-04\n",
      "6/6 - 0s - 16ms/step - loss: 259832784.0000 - mae: 259832784.0000 - rmse: 825318464.0000 - success_rate: 0.0000e+00\n",
      "Epoch 149/500\n",
      " - learning_rate: 8.028638e-04\n",
      "6/6 - 0s - 22ms/step - loss: 299163552.0000 - mae: 299163552.0000 - rmse: 1225558784.0000 - success_rate: 0.0000e+00\n",
      "Epoch 150/500\n",
      " - learning_rate: 8.003614e-04\n",
      "6/6 - 0s - 16ms/step - loss: 277869600.0000 - mae: 277869600.0000 - rmse: 880769600.0000 - success_rate: 0.0000e+00\n",
      "Epoch 151/500\n",
      " - learning_rate: 7.978471e-04\n",
      "6/6 - 0s - 16ms/step - loss: 253650704.0000 - mae: 253650704.0000 - rmse: 624493440.0000 - success_rate: 0.0000e+00\n",
      "Epoch 152/500\n",
      " - learning_rate: 7.953212e-04\n",
      "6/6 - 0s - 17ms/step - loss: 269951968.0000 - mae: 269951968.0000 - rmse: 1020796864.0000 - success_rate: 0.0000e+00\n",
      "Epoch 153/500\n",
      " - learning_rate: 7.927837e-04\n",
      "6/6 - 0s - 16ms/step - loss: 303194208.0000 - mae: 303194208.0000 - rmse: 992503872.0000 - success_rate: 0.0000e+00\n",
      "Epoch 154/500\n",
      " - learning_rate: 7.902347e-04\n",
      "6/6 - 0s - 26ms/step - loss: 280243168.0000 - mae: 280243168.0000 - rmse: 839911104.0000 - success_rate: 0.0000e+00\n",
      "Epoch 155/500\n",
      " - learning_rate: 7.876744e-04\n",
      "6/6 - 0s - 16ms/step - loss: 293120256.0000 - mae: 293120256.0000 - rmse: 1129774208.0000 - success_rate: 0.0000e+00\n",
      "Epoch 156/500\n",
      " - learning_rate: 7.851028e-04\n",
      "6/6 - 0s - 23ms/step - loss: 318587680.0000 - mae: 318587680.0000 - rmse: 1260788736.0000 - success_rate: 0.0000e+00\n",
      "Epoch 157/500\n",
      " - learning_rate: 7.825199e-04\n",
      "6/6 - 0s - 23ms/step - loss: 268424896.0000 - mae: 268424896.0000 - rmse: 1086634496.0000 - success_rate: 0.0000e+00\n",
      "Epoch 158/500\n",
      " - learning_rate: 7.799261e-04\n",
      "6/6 - 0s - 16ms/step - loss: 266689456.0000 - mae: 266689456.0000 - rmse: 778498240.0000 - success_rate: 0.0000e+00\n",
      "Epoch 159/500\n",
      " - learning_rate: 7.773213e-04\n",
      "6/6 - 0s - 16ms/step - loss: 302515136.0000 - mae: 302515136.0000 - rmse: 1385108480.0000 - success_rate: 0.0000e+00\n",
      "Epoch 160/500\n",
      " - learning_rate: 7.747056e-04\n",
      "6/6 - 0s - 23ms/step - loss: 285401280.0000 - mae: 285401280.0000 - rmse: 1099830400.0000 - success_rate: 0.0000e+00\n",
      "Epoch 161/500\n",
      " - learning_rate: 7.720792e-04\n",
      "6/6 - 0s - 16ms/step - loss: 313546112.0000 - mae: 313546112.0000 - rmse: 1235958528.0000 - success_rate: 0.0000e+00\n",
      "Epoch 162/500\n",
      " - learning_rate: 7.694422e-04\n",
      "6/6 - 0s - 16ms/step - loss: 274752576.0000 - mae: 274752576.0000 - rmse: 1054303552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 163/500\n",
      " - learning_rate: 7.667945e-04\n",
      "6/6 - 0s - 20ms/step - loss: 256866624.0000 - mae: 256866624.0000 - rmse: 780145920.0000 - success_rate: 0.0000e+00\n",
      "Epoch 164/500\n",
      " - learning_rate: 7.641364e-04\n",
      "6/6 - 0s - 17ms/step - loss: 282242720.0000 - mae: 282242720.0000 - rmse: 1031503360.0000 - success_rate: 0.0000e+00\n",
      "Epoch 165/500\n",
      " - learning_rate: 7.614680e-04\n",
      "6/6 - 0s - 16ms/step - loss: 308366368.0000 - mae: 308366368.0000 - rmse: 1372810368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 166/500\n",
      " - learning_rate: 7.587894e-04\n",
      "6/6 - 0s - 17ms/step - loss: 295447104.0000 - mae: 295447104.0000 - rmse: 1005894400.0000 - success_rate: 0.0000e+00\n",
      "Epoch 167/500\n",
      " - learning_rate: 7.561006e-04\n",
      "6/6 - 0s - 17ms/step - loss: 325274688.0000 - mae: 325274688.0000 - rmse: 1338719488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 168/500\n",
      " - learning_rate: 7.534018e-04\n",
      "6/6 - 0s - 16ms/step - loss: 257069088.0000 - mae: 257069088.0000 - rmse: 699304384.0000 - success_rate: 0.0000e+00\n",
      "Epoch 169/500\n",
      " - learning_rate: 7.506932e-04\n",
      "6/6 - 0s - 23ms/step - loss: 288837888.0000 - mae: 288837888.0000 - rmse: 1230958336.0000 - success_rate: 0.0000e+00\n",
      "Epoch 170/500\n",
      " - learning_rate: 7.479747e-04\n",
      "6/6 - 0s - 16ms/step - loss: 298672320.0000 - mae: 298672320.0000 - rmse: 980759552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 171/500\n",
      " - learning_rate: 7.452466e-04\n",
      "6/6 - 0s - 16ms/step - loss: 294181696.0000 - mae: 294181696.0000 - rmse: 1256199680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 172/500\n",
      " - learning_rate: 7.425089e-04\n",
      "6/6 - 0s - 20ms/step - loss: 267053264.0000 - mae: 267053264.0000 - rmse: 758291904.0000 - success_rate: 0.0000e+00\n",
      "Epoch 173/500\n",
      " - learning_rate: 7.397617e-04\n",
      "6/6 - 0s - 17ms/step - loss: 305457792.0000 - mae: 305457792.0000 - rmse: 1231876352.0000 - success_rate: 0.0000e+00\n",
      "Epoch 174/500\n",
      " - learning_rate: 7.370051e-04\n",
      "6/6 - 0s - 17ms/step - loss: 291330432.0000 - mae: 291330432.0000 - rmse: 1251004672.0000 - success_rate: 0.0000e+00\n",
      "Epoch 175/500\n",
      " - learning_rate: 7.342393e-04\n",
      "6/6 - 0s - 16ms/step - loss: 272886080.0000 - mae: 272886080.0000 - rmse: 1001418048.0000 - success_rate: 0.0000e+00\n",
      "Epoch 176/500\n",
      " - learning_rate: 7.314643e-04\n",
      "6/6 - 0s - 17ms/step - loss: 280893888.0000 - mae: 280893888.0000 - rmse: 1128178048.0000 - success_rate: 0.0000e+00\n",
      "Epoch 177/500\n",
      " - learning_rate: 7.286803e-04\n",
      "6/6 - 0s - 16ms/step - loss: 284404192.0000 - mae: 284404192.0000 - rmse: 1138539264.0000 - success_rate: 0.0000e+00\n",
      "Epoch 178/500\n",
      " - learning_rate: 7.258874e-04\n",
      "6/6 - 0s - 17ms/step - loss: 272997056.0000 - mae: 272997056.0000 - rmse: 1127925120.0000 - success_rate: 0.0000e+00\n",
      "Epoch 179/500\n",
      " - learning_rate: 7.230857e-04\n",
      "6/6 - 0s - 17ms/step - loss: 265746912.0000 - mae: 265746912.0000 - rmse: 931786176.0000 - success_rate: 0.0000e+00\n",
      "Epoch 180/500\n",
      " - learning_rate: 7.202753e-04\n",
      "6/6 - 0s - 27ms/step - loss: 250252816.0000 - mae: 250252816.0000 - rmse: 732298176.0000 - success_rate: 0.0000e+00\n",
      "Epoch 181/500\n",
      " - learning_rate: 7.174563e-04\n",
      "6/6 - 0s - 29ms/step - loss: 249981952.0000 - mae: 249981952.0000 - rmse: 719618816.0000 - success_rate: 0.0000e+00\n",
      "Epoch 182/500\n",
      " - learning_rate: 7.146288e-04\n",
      "6/6 - 0s - 29ms/step - loss: 226333728.0000 - mae: 226333728.0000 - rmse: 631362240.0000 - success_rate: 0.0000e+00\n",
      "Epoch 183/500\n",
      " - learning_rate: 7.117930e-04\n",
      "6/6 - 0s - 15ms/step - loss: 314099040.0000 - mae: 314099040.0000 - rmse: 1322244096.0000 - success_rate: 0.0000e+00\n",
      "Epoch 184/500\n",
      " - learning_rate: 7.089489e-04\n",
      "6/6 - 0s - 24ms/step - loss: 271106208.0000 - mae: 271106208.0000 - rmse: 796395840.0000 - success_rate: 0.0000e+00\n",
      "Epoch 185/500\n",
      " - learning_rate: 7.060967e-04\n",
      "6/6 - 0s - 23ms/step - loss: 288240384.0000 - mae: 288240384.0000 - rmse: 1046786944.0000 - success_rate: 0.0000e+00\n",
      "Epoch 186/500\n",
      " - learning_rate: 7.032365e-04\n",
      "6/6 - 0s - 16ms/step - loss: 289095968.0000 - mae: 289095968.0000 - rmse: 1188923264.0000 - success_rate: 0.0000e+00\n",
      "Epoch 187/500\n",
      " - learning_rate: 7.003684e-04\n",
      "6/6 - 0s - 16ms/step - loss: 278916032.0000 - mae: 278916032.0000 - rmse: 1033866560.0000 - success_rate: 0.0000e+00\n",
      "Epoch 188/500\n",
      " - learning_rate: 6.974924e-04\n",
      "6/6 - 0s - 16ms/step - loss: 263932432.0000 - mae: 263932432.0000 - rmse: 734030528.0000 - success_rate: 0.0000e+00\n",
      "Epoch 189/500\n",
      " - learning_rate: 6.946088e-04\n",
      "6/6 - 0s - 19ms/step - loss: 254333520.0000 - mae: 254333520.0000 - rmse: 748115136.0000 - success_rate: 0.0000e+00\n",
      "Epoch 190/500\n",
      " - learning_rate: 6.917177e-04\n",
      "6/6 - 0s - 20ms/step - loss: 281796992.0000 - mae: 281796992.0000 - rmse: 936196288.0000 - success_rate: 0.0000e+00\n",
      "Epoch 191/500\n",
      " - learning_rate: 6.888190e-04\n",
      "6/6 - 0s - 16ms/step - loss: 275676288.0000 - mae: 275676288.0000 - rmse: 1025339904.0000 - success_rate: 0.0000e+00\n",
      "Epoch 192/500\n",
      " - learning_rate: 6.859131e-04\n",
      "6/6 - 0s - 16ms/step - loss: 292238816.0000 - mae: 292238816.0000 - rmse: 1609742464.0000 - success_rate: 0.0000e+00\n",
      "Epoch 193/500\n",
      " - learning_rate: 6.830000e-04\n",
      "6/6 - 0s - 16ms/step - loss: 285586432.0000 - mae: 285586432.0000 - rmse: 1019499840.0000 - success_rate: 0.0000e+00\n",
      "Epoch 194/500\n",
      " - learning_rate: 6.800797e-04\n",
      "6/6 - 0s - 17ms/step - loss: 258961968.0000 - mae: 258961968.0000 - rmse: 966052288.0000 - success_rate: 0.0000e+00\n",
      "Epoch 195/500\n",
      " - learning_rate: 6.771525e-04\n",
      "6/6 - 0s - 16ms/step - loss: 297247360.0000 - mae: 297247360.0000 - rmse: 1276834560.0000 - success_rate: 0.0000e+00\n",
      "Epoch 196/500\n",
      " - learning_rate: 6.742185e-04\n",
      "6/6 - 0s - 15ms/step - loss: 297306688.0000 - mae: 297306688.0000 - rmse: 1546048384.0000 - success_rate: 0.0000e+00\n",
      "Epoch 197/500\n",
      " - learning_rate: 6.712776e-04\n",
      "6/6 - 0s - 16ms/step - loss: 282194976.0000 - mae: 282194976.0000 - rmse: 1159221120.0000 - success_rate: 0.0000e+00\n",
      "Epoch 198/500\n",
      " - learning_rate: 6.683302e-04\n",
      "6/6 - 0s - 16ms/step - loss: 241096880.0000 - mae: 241096880.0000 - rmse: 679898176.0000 - success_rate: 0.0000e+00\n",
      "Epoch 199/500\n",
      " - learning_rate: 6.653761e-04\n",
      "6/6 - 0s - 19ms/step - loss: 249771904.0000 - mae: 249771904.0000 - rmse: 808992576.0000 - success_rate: 0.0000e+00\n",
      "Epoch 200/500\n",
      " - learning_rate: 6.624158e-04\n",
      "6/6 - 0s - 17ms/step - loss: 282806592.0000 - mae: 282806592.0000 - rmse: 1312008832.0000 - success_rate: 0.0000e+00\n",
      "Epoch 201/500\n",
      " - learning_rate: 6.594491e-04\n",
      "6/6 - 0s - 16ms/step - loss: 292622496.0000 - mae: 292622496.0000 - rmse: 1135748224.0000 - success_rate: 0.0000e+00\n",
      "Epoch 202/500\n",
      " - learning_rate: 6.564763e-04\n",
      "6/6 - 0s - 16ms/step - loss: 257421712.0000 - mae: 257421712.0000 - rmse: 765964416.0000 - success_rate: 0.0000e+00\n",
      "Epoch 203/500\n",
      " - learning_rate: 6.534975e-04\n",
      "6/6 - 0s - 17ms/step - loss: 246384560.0000 - mae: 246384560.0000 - rmse: 770442944.0000 - success_rate: 0.0000e+00\n",
      "Epoch 204/500\n",
      " - learning_rate: 6.505127e-04\n",
      "6/6 - 0s - 16ms/step - loss: 267627312.0000 - mae: 267627312.0000 - rmse: 1084478208.0000 - success_rate: 0.0000e+00\n",
      "Epoch 205/500\n",
      " - learning_rate: 6.475221e-04\n",
      "6/6 - 0s - 22ms/step - loss: 268922592.0000 - mae: 268922592.0000 - rmse: 994221952.0000 - success_rate: 0.0000e+00\n",
      "Epoch 206/500\n",
      " - learning_rate: 6.445259e-04\n",
      "6/6 - 0s - 17ms/step - loss: 243606048.0000 - mae: 243606048.0000 - rmse: 746663680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 207/500\n",
      " - learning_rate: 6.415240e-04\n",
      "6/6 - 0s - 22ms/step - loss: 229319504.0000 - mae: 229319504.0000 - rmse: 835720576.0000 - success_rate: 0.0000e+00\n",
      "Epoch 208/500\n",
      " - learning_rate: 6.385167e-04\n",
      "6/6 - 0s - 23ms/step - loss: 258153488.0000 - mae: 258153488.0000 - rmse: 688318784.0000 - success_rate: 0.0000e+00\n",
      "Epoch 209/500\n",
      " - learning_rate: 6.355041e-04\n",
      "6/6 - 0s - 22ms/step - loss: 282351008.0000 - mae: 282351008.0000 - rmse: 1783503360.0000 - success_rate: 0.0000e+00\n",
      "Epoch 210/500\n",
      " - learning_rate: 6.324863e-04\n",
      "6/6 - 0s - 15ms/step - loss: 282025952.0000 - mae: 282025952.0000 - rmse: 1119262976.0000 - success_rate: 0.0000e+00\n",
      "Epoch 211/500\n",
      " - learning_rate: 6.294634e-04\n",
      "6/6 - 0s - 17ms/step - loss: 254991376.0000 - mae: 254991376.0000 - rmse: 830352768.0000 - success_rate: 0.0000e+00\n",
      "Epoch 212/500\n",
      " - learning_rate: 6.264356e-04\n",
      "6/6 - 0s - 33ms/step - loss: 208839984.0000 - mae: 208839984.0000 - rmse: 543585152.0000 - success_rate: 0.0000e+00\n",
      "Epoch 213/500\n",
      " - learning_rate: 6.234029e-04\n",
      "6/6 - 0s - 16ms/step - loss: 224875008.0000 - mae: 224875008.0000 - rmse: 672718528.0000 - success_rate: 0.0000e+00\n",
      "Epoch 214/500\n",
      " - learning_rate: 6.203656e-04\n",
      "6/6 - 0s - 16ms/step - loss: 241278880.0000 - mae: 241278880.0000 - rmse: 688472640.0000 - success_rate: 0.0000e+00\n",
      "Epoch 215/500\n",
      " - learning_rate: 6.173235e-04\n",
      "6/6 - 0s - 16ms/step - loss: 249277040.0000 - mae: 249277040.0000 - rmse: 984683392.0000 - success_rate: 0.0000e+00\n",
      "Epoch 216/500\n",
      " - learning_rate: 6.142771e-04\n",
      "6/6 - 0s - 26ms/step - loss: 239367008.0000 - mae: 239367008.0000 - rmse: 684618176.0000 - success_rate: 0.0000e+00\n",
      "Epoch 217/500\n",
      " - learning_rate: 6.112262e-04\n",
      "6/6 - 0s - 16ms/step - loss: 233801456.0000 - mae: 233801456.0000 - rmse: 817194048.0000 - success_rate: 0.0000e+00\n",
      "Epoch 218/500\n",
      " - learning_rate: 6.081712e-04\n",
      "6/6 - 0s - 16ms/step - loss: 268950208.0000 - mae: 268950208.0000 - rmse: 1025039552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 219/500\n",
      " - learning_rate: 6.051119e-04\n",
      "6/6 - 0s - 18ms/step - loss: 275865440.0000 - mae: 275865440.0000 - rmse: 1071046720.0000 - success_rate: 0.0000e+00\n",
      "Epoch 220/500\n",
      " - learning_rate: 6.020488e-04\n",
      "6/6 - 0s - 16ms/step - loss: 264015696.0000 - mae: 264015696.0000 - rmse: 1049972160.0000 - success_rate: 0.0000e+00\n",
      "Epoch 221/500\n",
      " - learning_rate: 5.989816e-04\n",
      "6/6 - 0s - 17ms/step - loss: 280099552.0000 - mae: 280099552.0000 - rmse: 1452719616.0000 - success_rate: 0.0000e+00\n",
      "Epoch 222/500\n",
      " - learning_rate: 5.959108e-04\n",
      "6/6 - 0s - 22ms/step - loss: 250133408.0000 - mae: 250133408.0000 - rmse: 818688640.0000 - success_rate: 0.0000e+00\n",
      "Epoch 223/500\n",
      " - learning_rate: 5.928364e-04\n",
      "6/6 - 0s - 16ms/step - loss: 257879888.0000 - mae: 257879888.0000 - rmse: 1079917312.0000 - success_rate: 0.0000e+00\n",
      "Epoch 224/500\n",
      " - learning_rate: 5.897585e-04\n",
      "6/6 - 0s - 16ms/step - loss: 261732320.0000 - mae: 261732320.0000 - rmse: 1201784960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 225/500\n",
      " - learning_rate: 5.866772e-04\n",
      "6/6 - 0s - 28ms/step - loss: 233986416.0000 - mae: 233986416.0000 - rmse: 818539200.0000 - success_rate: 0.0000e+00\n",
      "Epoch 226/500\n",
      " - learning_rate: 5.835926e-04\n",
      "6/6 - 0s - 27ms/step - loss: 221218032.0000 - mae: 221218032.0000 - rmse: 579817600.0000 - success_rate: 0.0000e+00\n",
      "Epoch 227/500\n",
      " - learning_rate: 5.805049e-04\n",
      "6/6 - 0s - 41ms/step - loss: 206771296.0000 - mae: 206771296.0000 - rmse: 482954080.0000 - success_rate: 0.0000e+00\n",
      "Epoch 228/500\n",
      " - learning_rate: 5.774142e-04\n",
      "6/6 - 0s - 26ms/step - loss: 266825264.0000 - mae: 266825264.0000 - rmse: 959474752.0000 - success_rate: 0.0000e+00\n",
      "Epoch 229/500\n",
      " - learning_rate: 5.743207e-04\n",
      "6/6 - 0s - 25ms/step - loss: 233204448.0000 - mae: 233204448.0000 - rmse: 692273536.0000 - success_rate: 0.0000e+00\n",
      "Epoch 230/500\n",
      " - learning_rate: 5.712243e-04\n",
      "6/6 - 0s - 26ms/step - loss: 270461376.0000 - mae: 270461376.0000 - rmse: 1195503616.0000 - success_rate: 0.0000e+00\n",
      "Epoch 231/500\n",
      " - learning_rate: 5.681252e-04\n",
      "6/6 - 0s - 28ms/step - loss: 276167648.0000 - mae: 276167648.0000 - rmse: 1156077440.0000 - success_rate: 0.0000e+00\n",
      "Epoch 232/500\n",
      " - learning_rate: 5.650237e-04\n",
      "6/6 - 0s - 25ms/step - loss: 264453680.0000 - mae: 264453680.0000 - rmse: 874249216.0000 - success_rate: 0.0000e+00\n",
      "Epoch 233/500\n",
      " - learning_rate: 5.619198e-04\n",
      "6/6 - 0s - 26ms/step - loss: 226200592.0000 - mae: 226200592.0000 - rmse: 624748992.0000 - success_rate: 0.0000e+00\n",
      "Epoch 234/500\n",
      " - learning_rate: 5.588137e-04\n",
      "6/6 - 0s - 24ms/step - loss: 246714688.0000 - mae: 246714688.0000 - rmse: 900521856.0000 - success_rate: 0.0000e+00\n",
      "Epoch 235/500\n",
      " - learning_rate: 5.557053e-04\n",
      "6/6 - 0s - 52ms/step - loss: 259698256.0000 - mae: 259698256.0000 - rmse: 895663872.0000 - success_rate: 0.0000e+00\n",
      "Epoch 236/500\n",
      " - learning_rate: 5.525950e-04\n",
      "6/6 - 0s - 51ms/step - loss: 238376240.0000 - mae: 238376240.0000 - rmse: 652959424.0000 - success_rate: 0.0000e+00\n",
      "Epoch 237/500\n",
      " - learning_rate: 5.494827e-04\n",
      "6/6 - 0s - 26ms/step - loss: 292297408.0000 - mae: 292297408.0000 - rmse: 1453606272.0000 - success_rate: 0.0000e+00\n",
      "Epoch 238/500\n",
      " - learning_rate: 5.463687e-04\n",
      "6/6 - 0s - 25ms/step - loss: 248270672.0000 - mae: 248270672.0000 - rmse: 1007371712.0000 - success_rate: 0.0000e+00\n",
      "Epoch 239/500\n",
      " - learning_rate: 5.432530e-04\n",
      "6/6 - 0s - 50ms/step - loss: 238177040.0000 - mae: 238177040.0000 - rmse: 745018624.0000 - success_rate: 0.0000e+00\n",
      "Epoch 240/500\n",
      " - learning_rate: 5.401358e-04\n",
      "6/6 - 0s - 27ms/step - loss: 229793040.0000 - mae: 229793040.0000 - rmse: 659880960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 241/500\n",
      " - learning_rate: 5.370172e-04\n",
      "6/6 - 0s - 28ms/step - loss: 242142608.0000 - mae: 242142608.0000 - rmse: 958515264.0000 - success_rate: 0.0000e+00\n",
      "Epoch 242/500\n",
      " - learning_rate: 5.338973e-04\n",
      "6/6 - 0s - 49ms/step - loss: 284087872.0000 - mae: 284087872.0000 - rmse: 1530167680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 243/500\n",
      " - learning_rate: 5.307763e-04\n",
      "6/6 - 0s - 27ms/step - loss: 245130480.0000 - mae: 245130480.0000 - rmse: 792603584.0000 - success_rate: 0.0000e+00\n",
      "Epoch 244/500\n",
      " - learning_rate: 5.276542e-04\n",
      "6/6 - 0s - 19ms/step - loss: 222742512.0000 - mae: 222742512.0000 - rmse: 749735488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 245/500\n",
      " - learning_rate: 5.245312e-04\n",
      "6/6 - 0s - 17ms/step - loss: 241232304.0000 - mae: 241232304.0000 - rmse: 991055552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 246/500\n",
      " - learning_rate: 5.214075e-04\n",
      "6/6 - 0s - 16ms/step - loss: 272422112.0000 - mae: 272422112.0000 - rmse: 1343757696.0000 - success_rate: 0.0000e+00\n",
      "Epoch 247/500\n",
      " - learning_rate: 5.182831e-04\n",
      "6/6 - 0s - 16ms/step - loss: 244032720.0000 - mae: 244032720.0000 - rmse: 937583936.0000 - success_rate: 0.0000e+00\n",
      "Epoch 248/500\n",
      " - learning_rate: 5.151582e-04\n",
      "6/6 - 0s - 17ms/step - loss: 269171648.0000 - mae: 269171648.0000 - rmse: 1117339648.0000 - success_rate: 0.0000e+00\n",
      "Epoch 249/500\n",
      " - learning_rate: 5.120329e-04\n",
      "6/6 - 0s - 20ms/step - loss: 222885376.0000 - mae: 222885376.0000 - rmse: 673134208.0000 - success_rate: 0.0000e+00\n",
      "Epoch 250/500\n",
      " - learning_rate: 5.089073e-04\n",
      "6/6 - 0s - 16ms/step - loss: 243915376.0000 - mae: 243915376.0000 - rmse: 904777216.0000 - success_rate: 0.0000e+00\n",
      "Epoch 251/500\n",
      " - learning_rate: 5.057815e-04\n",
      "6/6 - 0s - 16ms/step - loss: 253420528.0000 - mae: 253420528.0000 - rmse: 1025580544.0000 - success_rate: 0.0000e+00\n",
      "Epoch 252/500\n",
      " - learning_rate: 5.026557e-04\n",
      "6/6 - 0s - 16ms/step - loss: 226324496.0000 - mae: 226324496.0000 - rmse: 866420928.0000 - success_rate: 0.0000e+00\n",
      "Epoch 253/500\n",
      " - learning_rate: 4.995300e-04\n",
      "6/6 - 0s - 16ms/step - loss: 244713232.0000 - mae: 244713232.0000 - rmse: 932092224.0000 - success_rate: 0.0000e+00\n",
      "Epoch 254/500\n",
      " - learning_rate: 4.964045e-04\n",
      "6/6 - 0s - 16ms/step - loss: 221715472.0000 - mae: 221715472.0000 - rmse: 733259456.0000 - success_rate: 0.0000e+00\n",
      "Epoch 255/500\n",
      " - learning_rate: 4.932794e-04\n",
      "6/6 - 0s - 23ms/step - loss: 241915056.0000 - mae: 241915056.0000 - rmse: 1457200000.0000 - success_rate: 0.0000e+00\n",
      "Epoch 256/500\n",
      " - learning_rate: 4.901548e-04\n",
      "6/6 - 0s - 17ms/step - loss: 245418960.0000 - mae: 245418960.0000 - rmse: 1197681024.0000 - success_rate: 0.0000e+00\n",
      "Epoch 257/500\n",
      " - learning_rate: 4.870305e-04\n",
      "6/6 - 0s - 16ms/step - loss: 250467424.0000 - mae: 250467424.0000 - rmse: 1026054144.0000 - success_rate: 0.0000e+00\n",
      "Epoch 258/500\n",
      " - learning_rate: 4.839072e-04\n",
      "6/6 - 0s - 18ms/step - loss: 209699120.0000 - mae: 209699120.0000 - rmse: 654526464.0000 - success_rate: 0.0000e+00\n",
      "Epoch 259/500\n",
      " - learning_rate: 4.807847e-04\n",
      "6/6 - 0s - 18ms/step - loss: 223503712.0000 - mae: 223503712.0000 - rmse: 757054656.0000 - success_rate: 0.0000e+00\n",
      "Epoch 260/500\n",
      " - learning_rate: 4.776631e-04\n",
      "6/6 - 0s - 16ms/step - loss: 210684048.0000 - mae: 210684048.0000 - rmse: 607248896.0000 - success_rate: 0.0000e+00\n",
      "Epoch 261/500\n",
      " - learning_rate: 4.745426e-04\n",
      "6/6 - 0s - 23ms/step - loss: 248291984.0000 - mae: 248291984.0000 - rmse: 913097280.0000 - success_rate: 0.0000e+00\n",
      "Epoch 262/500\n",
      " - learning_rate: 4.714234e-04\n",
      "6/6 - 0s - 16ms/step - loss: 272414848.0000 - mae: 272414848.0000 - rmse: 1702418560.0000 - success_rate: 0.0000e+00\n",
      "Epoch 263/500\n",
      " - learning_rate: 4.683055e-04\n",
      "6/6 - 0s - 16ms/step - loss: 239042320.0000 - mae: 239042320.0000 - rmse: 970816064.0000 - success_rate: 0.0000e+00\n",
      "Epoch 264/500\n",
      " - learning_rate: 4.651890e-04\n",
      "6/6 - 0s - 23ms/step - loss: 249766736.0000 - mae: 249766736.0000 - rmse: 1098282624.0000 - success_rate: 0.0000e+00\n",
      "Epoch 265/500\n",
      " - learning_rate: 4.620741e-04\n",
      "6/6 - 0s - 23ms/step - loss: 263494752.0000 - mae: 263494752.0000 - rmse: 1228754048.0000 - success_rate: 0.0000e+00\n",
      "Epoch 266/500\n",
      " - learning_rate: 4.589610e-04\n",
      "6/6 - 0s - 16ms/step - loss: 249214944.0000 - mae: 249214944.0000 - rmse: 959348992.0000 - success_rate: 0.0000e+00\n",
      "Epoch 267/500\n",
      " - learning_rate: 4.558497e-04\n",
      "6/6 - 0s - 20ms/step - loss: 254114080.0000 - mae: 254114080.0000 - rmse: 927344128.0000 - success_rate: 0.0000e+00\n",
      "Epoch 268/500\n",
      " - learning_rate: 4.527403e-04\n",
      "6/6 - 0s - 16ms/step - loss: 259321664.0000 - mae: 259321664.0000 - rmse: 889954496.0000 - success_rate: 0.0000e+00\n",
      "Epoch 269/500\n",
      " - learning_rate: 4.496330e-04\n",
      "6/6 - 0s - 24ms/step - loss: 242040128.0000 - mae: 242040128.0000 - rmse: 881415872.0000 - success_rate: 0.0000e+00\n",
      "Epoch 270/500\n",
      " - learning_rate: 4.465280e-04\n",
      "6/6 - 0s - 16ms/step - loss: 259009872.0000 - mae: 259009872.0000 - rmse: 990105920.0000 - success_rate: 0.0000e+00\n",
      "Epoch 271/500\n",
      " - learning_rate: 4.434252e-04\n",
      "6/6 - 0s - 16ms/step - loss: 252970928.0000 - mae: 252970912.0000 - rmse: 859375488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 272/500\n",
      " - learning_rate: 4.403250e-04\n",
      "6/6 - 0s - 15ms/step - loss: 224856224.0000 - mae: 224856224.0000 - rmse: 788293312.0000 - success_rate: 0.0000e+00\n",
      "Epoch 273/500\n",
      " - learning_rate: 4.372273e-04\n",
      "6/6 - 0s - 17ms/step - loss: 234627376.0000 - mae: 234627376.0000 - rmse: 802496128.0000 - success_rate: 0.0000e+00\n",
      "Epoch 274/500\n",
      " - learning_rate: 4.341323e-04\n",
      "6/6 - 0s - 17ms/step - loss: 236278304.0000 - mae: 236278304.0000 - rmse: 800432960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 275/500\n",
      " - learning_rate: 4.310402e-04\n",
      "6/6 - 0s - 16ms/step - loss: 236672960.0000 - mae: 236672960.0000 - rmse: 918878784.0000 - success_rate: 0.0000e+00\n",
      "Epoch 276/500\n",
      " - learning_rate: 4.279509e-04\n",
      "6/6 - 0s - 25ms/step - loss: 223337168.0000 - mae: 223337168.0000 - rmse: 1169449856.0000 - success_rate: 0.0000e+00\n",
      "Epoch 277/500\n",
      " - learning_rate: 4.248648e-04\n",
      "6/6 - 0s - 17ms/step - loss: 229210800.0000 - mae: 229210800.0000 - rmse: 870967872.0000 - success_rate: 0.0000e+00\n",
      "Epoch 278/500\n",
      " - learning_rate: 4.217817e-04\n",
      "6/6 - 0s - 17ms/step - loss: 218960144.0000 - mae: 218960144.0000 - rmse: 683680512.0000 - success_rate: 0.0000e+00\n",
      "Epoch 279/500\n",
      " - learning_rate: 4.187021e-04\n",
      "6/6 - 0s - 16ms/step - loss: 216227520.0000 - mae: 216227520.0000 - rmse: 640088960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 280/500\n",
      " - learning_rate: 4.156259e-04\n",
      "6/6 - 0s - 16ms/step - loss: 234092240.0000 - mae: 234092240.0000 - rmse: 1066397504.0000 - success_rate: 0.0000e+00\n",
      "Epoch 281/500\n",
      " - learning_rate: 4.125533e-04\n",
      "6/6 - 0s - 16ms/step - loss: 241554480.0000 - mae: 241554480.0000 - rmse: 965496960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 282/500\n",
      " - learning_rate: 4.094843e-04\n",
      "6/6 - 0s - 19ms/step - loss: 224127728.0000 - mae: 224127728.0000 - rmse: 701555712.0000 - success_rate: 0.0000e+00\n",
      "Epoch 283/500\n",
      " - learning_rate: 4.064192e-04\n",
      "6/6 - 0s - 18ms/step - loss: 228345936.0000 - mae: 228345936.0000 - rmse: 854002176.0000 - success_rate: 0.0000e+00\n",
      "Epoch 284/500\n",
      " - learning_rate: 4.033580e-04\n",
      "6/6 - 0s - 16ms/step - loss: 238157280.0000 - mae: 238157280.0000 - rmse: 951734464.0000 - success_rate: 0.0000e+00\n",
      "Epoch 285/500\n",
      " - learning_rate: 4.003008e-04\n",
      "6/6 - 0s - 17ms/step - loss: 223123200.0000 - mae: 223123200.0000 - rmse: 678129984.0000 - success_rate: 0.0000e+00\n",
      "Epoch 286/500\n",
      " - learning_rate: 3.972478e-04\n",
      "6/6 - 0s - 20ms/step - loss: 224319008.0000 - mae: 224319008.0000 - rmse: 754985920.0000 - success_rate: 0.0000e+00\n",
      "Epoch 287/500\n",
      " - learning_rate: 3.941992e-04\n",
      "6/6 - 0s - 20ms/step - loss: 210062640.0000 - mae: 210062640.0000 - rmse: 711177088.0000 - success_rate: 0.0000e+00\n",
      "Epoch 288/500\n",
      " - learning_rate: 3.911550e-04\n",
      "6/6 - 0s - 17ms/step - loss: 255240960.0000 - mae: 255240960.0000 - rmse: 909678144.0000 - success_rate: 0.0000e+00\n",
      "Epoch 289/500\n",
      " - learning_rate: 3.881153e-04\n",
      "6/6 - 0s - 16ms/step - loss: 225554240.0000 - mae: 225554240.0000 - rmse: 610763584.0000 - success_rate: 0.0000e+00\n",
      "Epoch 290/500\n",
      " - learning_rate: 3.850802e-04\n",
      "6/6 - 0s - 15ms/step - loss: 228640992.0000 - mae: 228640992.0000 - rmse: 998292288.0000 - success_rate: 0.0000e+00\n",
      "Epoch 291/500\n",
      " - learning_rate: 3.820499e-04\n",
      "6/6 - 0s - 23ms/step - loss: 222860864.0000 - mae: 222860864.0000 - rmse: 692081856.0000 - success_rate: 0.0000e+00\n",
      "Epoch 292/500\n",
      " - learning_rate: 3.790245e-04\n",
      "6/6 - 0s - 17ms/step - loss: 241965952.0000 - mae: 241965952.0000 - rmse: 971382464.0000 - success_rate: 0.0000e+00\n",
      "Epoch 293/500\n",
      " - learning_rate: 3.760042e-04\n",
      "6/6 - 0s - 17ms/step - loss: 235578944.0000 - mae: 235578944.0000 - rmse: 1018249856.0000 - success_rate: 0.0000e+00\n",
      "Epoch 294/500\n",
      " - learning_rate: 3.729890e-04\n",
      "6/6 - 0s - 16ms/step - loss: 217460864.0000 - mae: 217460864.0000 - rmse: 734163520.0000 - success_rate: 0.0000e+00\n",
      "Epoch 295/500\n",
      " - learning_rate: 3.699790e-04\n",
      "6/6 - 0s - 25ms/step - loss: 233993600.0000 - mae: 233993600.0000 - rmse: 850063232.0000 - success_rate: 0.0000e+00\n",
      "Epoch 296/500\n",
      " - learning_rate: 3.669745e-04\n",
      "6/6 - 0s - 16ms/step - loss: 248652864.0000 - mae: 248652864.0000 - rmse: 1046428992.0000 - success_rate: 0.0000e+00\n",
      "Epoch 297/500\n",
      " - learning_rate: 3.639754e-04\n",
      "6/6 - 0s - 16ms/step - loss: 208699888.0000 - mae: 208699888.0000 - rmse: 758366720.0000 - success_rate: 0.0000e+00\n",
      "Epoch 298/500\n",
      " - learning_rate: 3.609820e-04\n",
      "6/6 - 0s - 18ms/step - loss: 238952208.0000 - mae: 238952208.0000 - rmse: 916864704.0000 - success_rate: 0.0000e+00\n",
      "Epoch 299/500\n",
      " - learning_rate: 3.579942e-04\n",
      "6/6 - 0s - 22ms/step - loss: 249754992.0000 - mae: 249754992.0000 - rmse: 905158016.0000 - success_rate: 0.0000e+00\n",
      "Epoch 300/500\n",
      " - learning_rate: 3.550124e-04\n",
      "6/6 - 0s - 17ms/step - loss: 233901312.0000 - mae: 233901312.0000 - rmse: 841113088.0000 - success_rate: 0.0000e+00\n",
      "Epoch 301/500\n",
      " - learning_rate: 3.520365e-04\n",
      "6/6 - 0s - 16ms/step - loss: 252585376.0000 - mae: 252585376.0000 - rmse: 947371776.0000 - success_rate: 0.0000e+00\n",
      "Epoch 302/500\n",
      " - learning_rate: 3.490668e-04\n",
      "6/6 - 0s - 16ms/step - loss: 240233552.0000 - mae: 240233552.0000 - rmse: 862738432.0000 - success_rate: 0.0000e+00\n",
      "Epoch 303/500\n",
      " - learning_rate: 3.461032e-04\n",
      "6/6 - 0s - 16ms/step - loss: 256104480.0000 - mae: 256104480.0000 - rmse: 1216766976.0000 - success_rate: 0.0000e+00\n",
      "Epoch 304/500\n",
      " - learning_rate: 3.431460e-04\n",
      "6/6 - 0s - 26ms/step - loss: 264095920.0000 - mae: 264095920.0000 - rmse: 1178266624.0000 - success_rate: 0.0000e+00\n",
      "Epoch 305/500\n",
      " - learning_rate: 3.401954e-04\n",
      "6/6 - 0s - 16ms/step - loss: 221081312.0000 - mae: 221081312.0000 - rmse: 682343680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 306/500\n",
      " - learning_rate: 3.372512e-04\n",
      "6/6 - 0s - 16ms/step - loss: 220846976.0000 - mae: 220846976.0000 - rmse: 685779456.0000 - success_rate: 0.0000e+00\n",
      "Epoch 307/500\n",
      " - learning_rate: 3.343138e-04\n",
      "6/6 - 0s - 15ms/step - loss: 226533232.0000 - mae: 226533232.0000 - rmse: 766828096.0000 - success_rate: 0.0000e+00\n",
      "Epoch 308/500\n",
      " - learning_rate: 3.313831e-04\n",
      "6/6 - 0s - 27ms/step - loss: 199427072.0000 - mae: 199427072.0000 - rmse: 487388416.0000 - success_rate: 0.0000e+00\n",
      "Epoch 309/500\n",
      " - learning_rate: 3.284593e-04\n",
      "6/6 - 0s - 16ms/step - loss: 224701728.0000 - mae: 224701728.0000 - rmse: 794042816.0000 - success_rate: 0.0000e+00\n",
      "Epoch 310/500\n",
      " - learning_rate: 3.255426e-04\n",
      "6/6 - 0s - 16ms/step - loss: 215425712.0000 - mae: 215425712.0000 - rmse: 706324096.0000 - success_rate: 0.0000e+00\n",
      "Epoch 311/500\n",
      " - learning_rate: 3.226330e-04\n",
      "6/6 - 0s - 16ms/step - loss: 226317488.0000 - mae: 226317488.0000 - rmse: 855608192.0000 - success_rate: 0.0000e+00\n",
      "Epoch 312/500\n",
      " - learning_rate: 3.197308e-04\n",
      "6/6 - 0s - 16ms/step - loss: 232591504.0000 - mae: 232591504.0000 - rmse: 824479552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 313/500\n",
      " - learning_rate: 3.168359e-04\n",
      "6/6 - 0s - 19ms/step - loss: 228114272.0000 - mae: 228114272.0000 - rmse: 944239744.0000 - success_rate: 0.0000e+00\n",
      "Epoch 314/500\n",
      " - learning_rate: 3.139485e-04\n",
      "6/6 - 0s - 16ms/step - loss: 222113792.0000 - mae: 222113792.0000 - rmse: 1051377152.0000 - success_rate: 0.0000e+00\n",
      "Epoch 315/500\n",
      " - learning_rate: 3.110687e-04\n",
      "6/6 - 0s - 27ms/step - loss: 193237008.0000 - mae: 193237008.0000 - rmse: 605806720.0000 - success_rate: 0.0000e+00\n",
      "Epoch 316/500\n",
      " - learning_rate: 3.081967e-04\n",
      "6/6 - 0s - 16ms/step - loss: 201093680.0000 - mae: 201093680.0000 - rmse: 561670336.0000 - success_rate: 0.0000e+00\n",
      "Epoch 317/500\n",
      " - learning_rate: 3.053325e-04\n",
      "6/6 - 0s - 16ms/step - loss: 213078032.0000 - mae: 213078032.0000 - rmse: 651119552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 318/500\n",
      " - learning_rate: 3.024763e-04\n",
      "6/6 - 0s - 23ms/step - loss: 209881488.0000 - mae: 209881488.0000 - rmse: 701407104.0000 - success_rate: 0.0000e+00\n",
      "Epoch 319/500\n",
      " - learning_rate: 2.996281e-04\n",
      "6/6 - 0s - 16ms/step - loss: 209167248.0000 - mae: 209167248.0000 - rmse: 720063168.0000 - success_rate: 0.0000e+00\n",
      "Epoch 320/500\n",
      " - learning_rate: 2.967882e-04\n",
      "6/6 - 0s - 17ms/step - loss: 261698368.0000 - mae: 261698368.0000 - rmse: 1153832832.0000 - success_rate: 0.0000e+00\n",
      "Epoch 321/500\n",
      " - learning_rate: 2.939564e-04\n",
      "6/6 - 0s - 16ms/step - loss: 220409616.0000 - mae: 220409616.0000 - rmse: 740699776.0000 - success_rate: 0.0000e+00\n",
      "Epoch 322/500\n",
      " - learning_rate: 2.911332e-04\n",
      "6/6 - 0s - 19ms/step - loss: 236797696.0000 - mae: 236797696.0000 - rmse: 975813440.0000 - success_rate: 0.0000e+00\n",
      "Epoch 323/500\n",
      " - learning_rate: 2.883184e-04\n",
      "6/6 - 0s - 16ms/step - loss: 227850640.0000 - mae: 227850640.0000 - rmse: 760182464.0000 - success_rate: 0.0000e+00\n",
      "Epoch 324/500\n",
      " - learning_rate: 2.855123e-04\n",
      "6/6 - 0s - 16ms/step - loss: 229730592.0000 - mae: 229730592.0000 - rmse: 864711488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 325/500\n",
      " - learning_rate: 2.827150e-04\n",
      "6/6 - 0s - 15ms/step - loss: 218593552.0000 - mae: 218593552.0000 - rmse: 719576768.0000 - success_rate: 0.0000e+00\n",
      "Epoch 326/500\n",
      " - learning_rate: 2.799266e-04\n",
      "6/6 - 0s - 17ms/step - loss: 213695840.0000 - mae: 213695840.0000 - rmse: 775283008.0000 - success_rate: 0.0000e+00\n",
      "Epoch 327/500\n",
      " - learning_rate: 2.771471e-04\n",
      "6/6 - 0s - 15ms/step - loss: 224365872.0000 - mae: 224365872.0000 - rmse: 890172608.0000 - success_rate: 0.0000e+00\n",
      "Epoch 328/500\n",
      " - learning_rate: 2.743767e-04\n",
      "6/6 - 0s - 23ms/step - loss: 207933824.0000 - mae: 207933824.0000 - rmse: 601369280.0000 - success_rate: 0.0000e+00\n",
      "Epoch 329/500\n",
      " - learning_rate: 2.716155e-04\n",
      "6/6 - 0s - 16ms/step - loss: 234937088.0000 - mae: 234937088.0000 - rmse: 854156992.0000 - success_rate: 0.0000e+00\n",
      "Epoch 330/500\n",
      " - learning_rate: 2.688636e-04\n",
      "6/6 - 0s - 16ms/step - loss: 204378176.0000 - mae: 204378176.0000 - rmse: 621724416.0000 - success_rate: 0.0000e+00\n",
      "Epoch 331/500\n",
      " - learning_rate: 2.661211e-04\n",
      "6/6 - 0s - 16ms/step - loss: 234802240.0000 - mae: 234802240.0000 - rmse: 854452224.0000 - success_rate: 0.0000e+00\n",
      "Epoch 332/500\n",
      " - learning_rate: 2.633882e-04\n",
      "6/6 - 0s - 25ms/step - loss: 220022592.0000 - mae: 220022592.0000 - rmse: 743095488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 333/500\n",
      " - learning_rate: 2.606648e-04\n",
      "6/6 - 0s - 26ms/step - loss: 236083568.0000 - mae: 236083568.0000 - rmse: 852760128.0000 - success_rate: 0.0000e+00\n",
      "Epoch 334/500\n",
      " - learning_rate: 2.579512e-04\n",
      "6/6 - 0s - 29ms/step - loss: 231229488.0000 - mae: 231229488.0000 - rmse: 801775616.0000 - success_rate: 0.0000e+00\n",
      "Epoch 335/500\n",
      " - learning_rate: 2.552475e-04\n",
      "6/6 - 0s - 25ms/step - loss: 212440336.0000 - mae: 212440336.0000 - rmse: 792335360.0000 - success_rate: 0.0000e+00\n",
      "Epoch 336/500\n",
      " - learning_rate: 2.525538e-04\n",
      "6/6 - 0s - 50ms/step - loss: 210381456.0000 - mae: 210381456.0000 - rmse: 756255296.0000 - success_rate: 0.0000e+00\n",
      "Epoch 337/500\n",
      " - learning_rate: 2.498702e-04\n",
      "6/6 - 0s - 50ms/step - loss: 223792128.0000 - mae: 223792128.0000 - rmse: 825060864.0000 - success_rate: 0.0000e+00\n",
      "Epoch 338/500\n",
      " - learning_rate: 2.471966e-04\n",
      "6/6 - 0s - 27ms/step - loss: 201967328.0000 - mae: 201967328.0000 - rmse: 610256576.0000 - success_rate: 0.0000e+00\n",
      "Epoch 339/500\n",
      " - learning_rate: 2.445334e-04\n",
      "6/6 - 0s - 25ms/step - loss: 213758880.0000 - mae: 213758880.0000 - rmse: 734496640.0000 - success_rate: 0.0000e+00\n",
      "Epoch 340/500\n",
      " - learning_rate: 2.418806e-04\n",
      "6/6 - 0s - 50ms/step - loss: 233718656.0000 - mae: 233718656.0000 - rmse: 1282672000.0000 - success_rate: 0.0000e+00\n",
      "Epoch 341/500\n",
      " - learning_rate: 2.392382e-04\n",
      "6/6 - 0s - 25ms/step - loss: 211833712.0000 - mae: 211833712.0000 - rmse: 748982784.0000 - success_rate: 0.0000e+00\n",
      "Epoch 342/500\n",
      " - learning_rate: 2.366063e-04\n",
      "6/6 - 0s - 50ms/step - loss: 199575936.0000 - mae: 199575936.0000 - rmse: 643621632.0000 - success_rate: 0.0000e+00\n",
      "Epoch 343/500\n",
      " - learning_rate: 2.339853e-04\n",
      "6/6 - 0s - 41ms/step - loss: 189861168.0000 - mae: 189861168.0000 - rmse: 526556960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 344/500\n",
      " - learning_rate: 2.313750e-04\n",
      "6/6 - 0s - 35ms/step - loss: 217017984.0000 - mae: 217017984.0000 - rmse: 679648256.0000 - success_rate: 0.0000e+00\n",
      "Epoch 345/500\n",
      " - learning_rate: 2.287756e-04\n",
      "6/6 - 0s - 26ms/step - loss: 236204032.0000 - mae: 236204032.0000 - rmse: 1055710336.0000 - success_rate: 0.0000e+00\n",
      "Epoch 346/500\n",
      " - learning_rate: 2.261873e-04\n",
      "6/6 - 0s - 28ms/step - loss: 204596016.0000 - mae: 204596016.0000 - rmse: 718651648.0000 - success_rate: 0.0000e+00\n",
      "Epoch 347/500\n",
      " - learning_rate: 2.236101e-04\n",
      "6/6 - 0s - 49ms/step - loss: 195739824.0000 - mae: 195739824.0000 - rmse: 625618752.0000 - success_rate: 0.0000e+00\n",
      "Epoch 348/500\n",
      " - learning_rate: 2.210441e-04\n",
      "6/6 - 0s - 24ms/step - loss: 215530160.0000 - mae: 215530160.0000 - rmse: 682835712.0000 - success_rate: 0.0000e+00\n",
      "Epoch 349/500\n",
      " - learning_rate: 2.184894e-04\n",
      "6/6 - 0s - 16ms/step - loss: 239323424.0000 - mae: 239323424.0000 - rmse: 975808512.0000 - success_rate: 0.0000e+00\n",
      "Epoch 350/500\n",
      " - learning_rate: 2.159462e-04\n",
      "6/6 - 0s - 16ms/step - loss: 219404176.0000 - mae: 219404176.0000 - rmse: 865079296.0000 - success_rate: 0.0000e+00\n",
      "Epoch 351/500\n",
      " - learning_rate: 2.134145e-04\n",
      "6/6 - 0s - 16ms/step - loss: 201243568.0000 - mae: 201243568.0000 - rmse: 585730560.0000 - success_rate: 0.0000e+00\n",
      "Epoch 352/500\n",
      " - learning_rate: 2.108943e-04\n",
      "6/6 - 0s - 27ms/step - loss: 187167504.0000 - mae: 187167504.0000 - rmse: 516369184.0000 - success_rate: 0.0000e+00\n",
      "Epoch 353/500\n",
      " - learning_rate: 2.083860e-04\n",
      "6/6 - 0s - 17ms/step - loss: 216229392.0000 - mae: 216229392.0000 - rmse: 1206646912.0000 - success_rate: 0.0000e+00\n",
      "Epoch 354/500\n",
      " - learning_rate: 2.058894e-04\n",
      "6/6 - 0s - 18ms/step - loss: 205041968.0000 - mae: 205041968.0000 - rmse: 619517120.0000 - success_rate: 0.0000e+00\n",
      "Epoch 355/500\n",
      " - learning_rate: 2.034048e-04\n",
      "6/6 - 0s - 23ms/step - loss: 206164752.0000 - mae: 206164752.0000 - rmse: 695326528.0000 - success_rate: 0.0000e+00\n",
      "Epoch 356/500\n",
      " - learning_rate: 2.009322e-04\n",
      "6/6 - 0s - 22ms/step - loss: 212582480.0000 - mae: 212582480.0000 - rmse: 764498496.0000 - success_rate: 0.0000e+00\n",
      "Epoch 357/500\n",
      " - learning_rate: 1.984718e-04\n",
      "6/6 - 0s - 23ms/step - loss: 213663712.0000 - mae: 213663712.0000 - rmse: 923096128.0000 - success_rate: 0.0000e+00\n",
      "Epoch 358/500\n",
      " - learning_rate: 1.960236e-04\n",
      "6/6 - 0s - 16ms/step - loss: 200474336.0000 - mae: 200474336.0000 - rmse: 589479680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 359/500\n",
      " - learning_rate: 1.935876e-04\n",
      "6/6 - 0s - 16ms/step - loss: 207637696.0000 - mae: 207637696.0000 - rmse: 796969984.0000 - success_rate: 0.0000e+00\n",
      "Epoch 360/500\n",
      " - learning_rate: 1.911641e-04\n",
      "6/6 - 0s - 16ms/step - loss: 212611024.0000 - mae: 212611024.0000 - rmse: 853762304.0000 - success_rate: 0.0000e+00\n",
      "Epoch 361/500\n",
      " - learning_rate: 1.887532e-04\n",
      "6/6 - 0s - 17ms/step - loss: 210318448.0000 - mae: 210318448.0000 - rmse: 753790144.0000 - success_rate: 0.0000e+00\n",
      "Epoch 362/500\n",
      " - learning_rate: 1.863548e-04\n",
      "6/6 - 0s - 18ms/step - loss: 233465520.0000 - mae: 233465520.0000 - rmse: 959423168.0000 - success_rate: 0.0000e+00\n",
      "Epoch 363/500\n",
      " - learning_rate: 1.839690e-04\n",
      "6/6 - 0s - 18ms/step - loss: 201423536.0000 - mae: 201423536.0000 - rmse: 796420544.0000 - success_rate: 0.0000e+00\n",
      "Epoch 364/500\n",
      " - learning_rate: 1.815962e-04\n",
      "6/6 - 0s - 17ms/step - loss: 228139312.0000 - mae: 228139312.0000 - rmse: 1029533760.0000 - success_rate: 0.0000e+00\n",
      "Epoch 365/500\n",
      " - learning_rate: 1.792362e-04\n",
      "6/6 - 0s - 16ms/step - loss: 238389072.0000 - mae: 238389072.0000 - rmse: 949982528.0000 - success_rate: 0.0000e+00\n",
      "Epoch 366/500\n",
      " - learning_rate: 1.768892e-04\n",
      "6/6 - 0s - 17ms/step - loss: 200147376.0000 - mae: 200147376.0000 - rmse: 571065152.0000 - success_rate: 0.0000e+00\n",
      "Epoch 367/500\n",
      " - learning_rate: 1.745553e-04\n",
      "6/6 - 0s - 16ms/step - loss: 216495872.0000 - mae: 216495872.0000 - rmse: 752780544.0000 - success_rate: 0.0000e+00\n",
      "Epoch 368/500\n",
      " - learning_rate: 1.722346e-04\n",
      "6/6 - 0s - 16ms/step - loss: 199763744.0000 - mae: 199763744.0000 - rmse: 714382272.0000 - success_rate: 0.0000e+00\n",
      "Epoch 369/500\n",
      " - learning_rate: 1.699272e-04\n",
      "6/6 - 0s - 23ms/step - loss: 222625392.0000 - mae: 222625392.0000 - rmse: 833432448.0000 - success_rate: 0.0000e+00\n",
      "Epoch 370/500\n",
      " - learning_rate: 1.676332e-04\n",
      "6/6 - 0s - 16ms/step - loss: 230488336.0000 - mae: 230488336.0000 - rmse: 816796928.0000 - success_rate: 0.0000e+00\n",
      "Epoch 371/500\n",
      " - learning_rate: 1.653525e-04\n",
      "6/6 - 0s - 17ms/step - loss: 203165248.0000 - mae: 203165248.0000 - rmse: 592064896.0000 - success_rate: 0.0000e+00\n",
      "Epoch 372/500\n",
      " - learning_rate: 1.630854e-04\n",
      "6/6 - 0s - 19ms/step - loss: 220975696.0000 - mae: 220975696.0000 - rmse: 804019520.0000 - success_rate: 0.0000e+00\n",
      "Epoch 373/500\n",
      " - learning_rate: 1.608320e-04\n",
      "6/6 - 0s - 17ms/step - loss: 205399584.0000 - mae: 205399584.0000 - rmse: 598228032.0000 - success_rate: 0.0000e+00\n",
      "Epoch 374/500\n",
      " - learning_rate: 1.585923e-04\n",
      "6/6 - 0s - 16ms/step - loss: 221904000.0000 - mae: 221904000.0000 - rmse: 986221568.0000 - success_rate: 0.0000e+00\n",
      "Epoch 375/500\n",
      " - learning_rate: 1.563663e-04\n",
      "6/6 - 0s - 16ms/step - loss: 195213360.0000 - mae: 195213360.0000 - rmse: 612290176.0000 - success_rate: 0.0000e+00\n",
      "Epoch 376/500\n",
      " - learning_rate: 1.541543e-04\n",
      "6/6 - 0s - 22ms/step - loss: 201400960.0000 - mae: 201400960.0000 - rmse: 626749632.0000 - success_rate: 0.0000e+00\n",
      "Epoch 377/500\n",
      " - learning_rate: 1.519563e-04\n",
      "6/6 - 0s - 16ms/step - loss: 206071568.0000 - mae: 206071568.0000 - rmse: 688347968.0000 - success_rate: 0.0000e+00\n",
      "Epoch 378/500\n",
      " - learning_rate: 1.497723e-04\n",
      "6/6 - 0s - 16ms/step - loss: 207546656.0000 - mae: 207546656.0000 - rmse: 761063872.0000 - success_rate: 0.0000e+00\n",
      "Epoch 379/500\n",
      " - learning_rate: 1.476025e-04\n",
      "6/6 - 0s - 16ms/step - loss: 195477792.0000 - mae: 195477792.0000 - rmse: 571647488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 380/500\n",
      " - learning_rate: 1.454470e-04\n",
      "6/6 - 0s - 16ms/step - loss: 210775424.0000 - mae: 210775424.0000 - rmse: 665441344.0000 - success_rate: 0.0000e+00\n",
      "Epoch 381/500\n",
      " - learning_rate: 1.433058e-04\n",
      "6/6 - 0s - 19ms/step - loss: 220714160.0000 - mae: 220714160.0000 - rmse: 895323520.0000 - success_rate: 0.0000e+00\n",
      "Epoch 382/500\n",
      " - learning_rate: 1.411790e-04\n",
      "6/6 - 0s - 19ms/step - loss: 207279360.0000 - mae: 207279360.0000 - rmse: 648945792.0000 - success_rate: 0.0000e+00\n",
      "Epoch 383/500\n",
      " - learning_rate: 1.390668e-04\n",
      "6/6 - 0s - 22ms/step - loss: 201645664.0000 - mae: 201645664.0000 - rmse: 722614720.0000 - success_rate: 0.0000e+00\n",
      "Epoch 384/500\n",
      " - learning_rate: 1.369690e-04\n",
      "6/6 - 0s - 27ms/step - loss: 184738112.0000 - mae: 184738112.0000 - rmse: 488318944.0000 - success_rate: 0.0000e+00\n",
      "Epoch 385/500\n",
      " - learning_rate: 1.348861e-04\n",
      "6/6 - 0s - 16ms/step - loss: 195257840.0000 - mae: 195257840.0000 - rmse: 726215040.0000 - success_rate: 0.0000e+00\n",
      "Epoch 386/500\n",
      " - learning_rate: 1.328178e-04\n",
      "6/6 - 0s - 16ms/step - loss: 196380464.0000 - mae: 196380464.0000 - rmse: 515617120.0000 - success_rate: 0.0000e+00\n",
      "Epoch 387/500\n",
      " - learning_rate: 1.307644e-04\n",
      "6/6 - 0s - 16ms/step - loss: 205248272.0000 - mae: 205248272.0000 - rmse: 749659840.0000 - success_rate: 0.0000e+00\n",
      "Epoch 388/500\n",
      " - learning_rate: 1.287259e-04\n",
      "6/6 - 0s - 17ms/step - loss: 201690144.0000 - mae: 201690144.0000 - rmse: 677326208.0000 - success_rate: 0.0000e+00\n",
      "Epoch 389/500\n",
      " - learning_rate: 1.267025e-04\n",
      "6/6 - 0s - 17ms/step - loss: 202616432.0000 - mae: 202616432.0000 - rmse: 570727232.0000 - success_rate: 0.0000e+00\n",
      "Epoch 390/500\n",
      " - learning_rate: 1.246941e-04\n",
      "6/6 - 0s - 19ms/step - loss: 201811424.0000 - mae: 201811424.0000 - rmse: 757592320.0000 - success_rate: 0.0000e+00\n",
      "Epoch 391/500\n",
      " - learning_rate: 1.227009e-04\n",
      "6/6 - 0s - 18ms/step - loss: 215700752.0000 - mae: 215700752.0000 - rmse: 690542080.0000 - success_rate: 0.0000e+00\n",
      "Epoch 392/500\n",
      " - learning_rate: 1.207229e-04\n",
      "6/6 - 0s - 16ms/step - loss: 201119408.0000 - mae: 201119408.0000 - rmse: 836505792.0000 - success_rate: 0.0000e+00\n",
      "Epoch 393/500\n",
      " - learning_rate: 1.187602e-04\n",
      "6/6 - 0s - 17ms/step - loss: 210410592.0000 - mae: 210410592.0000 - rmse: 663907200.0000 - success_rate: 0.0000e+00\n",
      "Epoch 394/500\n",
      " - learning_rate: 1.168130e-04\n",
      "6/6 - 0s - 16ms/step - loss: 231272432.0000 - mae: 231272432.0000 - rmse: 928383680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 395/500\n",
      " - learning_rate: 1.148812e-04\n",
      "6/6 - 0s - 16ms/step - loss: 212985856.0000 - mae: 212985856.0000 - rmse: 604320640.0000 - success_rate: 0.0000e+00\n",
      "Epoch 396/500\n",
      " - learning_rate: 1.129650e-04\n",
      "6/6 - 0s - 16ms/step - loss: 187478272.0000 - mae: 187478272.0000 - rmse: 483282016.0000 - success_rate: 0.0000e+00\n",
      "Epoch 397/500\n",
      " - learning_rate: 1.110644e-04\n",
      "6/6 - 0s - 16ms/step - loss: 195446224.0000 - mae: 195446224.0000 - rmse: 616085056.0000 - success_rate: 0.0000e+00\n",
      "Epoch 398/500\n",
      " - learning_rate: 1.091795e-04\n",
      "6/6 - 0s - 16ms/step - loss: 190573968.0000 - mae: 190573968.0000 - rmse: 666930240.0000 - success_rate: 0.0000e+00\n",
      "Epoch 399/500\n",
      " - learning_rate: 1.073104e-04\n",
      "6/6 - 0s - 17ms/step - loss: 192510336.0000 - mae: 192510336.0000 - rmse: 594510848.0000 - success_rate: 0.0000e+00\n",
      "Epoch 400/500\n",
      " - learning_rate: 1.054572e-04\n",
      "6/6 - 0s - 20ms/step - loss: 198888144.0000 - mae: 198888144.0000 - rmse: 641382656.0000 - success_rate: 0.0000e+00\n",
      "Epoch 401/500\n",
      " - learning_rate: 1.036199e-04\n",
      "6/6 - 0s - 19ms/step - loss: 221601568.0000 - mae: 221601568.0000 - rmse: 780808192.0000 - success_rate: 0.0000e+00\n",
      "Epoch 402/500\n",
      " - learning_rate: 1.017986e-04\n",
      "6/6 - 0s - 16ms/step - loss: 198024992.0000 - mae: 198024992.0000 - rmse: 654956608.0000 - success_rate: 0.0000e+00\n",
      "Epoch 403/500\n",
      " - learning_rate: 9.999340e-05\n",
      "6/6 - 0s - 15ms/step - loss: 210618448.0000 - mae: 210618448.0000 - rmse: 910071488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 404/500\n",
      " - learning_rate: 9.820434e-05\n",
      "6/6 - 0s - 16ms/step - loss: 203558064.0000 - mae: 203558064.0000 - rmse: 685196736.0000 - success_rate: 0.0000e+00\n",
      "Epoch 405/500\n",
      " - learning_rate: 9.643150e-05\n",
      "6/6 - 0s - 16ms/step - loss: 202235104.0000 - mae: 202235104.0000 - rmse: 791602624.0000 - success_rate: 0.0000e+00\n",
      "Epoch 406/500\n",
      " - learning_rate: 9.467495e-05\n",
      "6/6 - 0s - 17ms/step - loss: 185345136.0000 - mae: 185345136.0000 - rmse: 546572480.0000 - success_rate: 0.0000e+00\n",
      "Epoch 407/500\n",
      " - learning_rate: 9.293476e-05\n",
      "6/6 - 0s - 27ms/step - loss: 184018624.0000 - mae: 184018624.0000 - rmse: 580846400.0000 - success_rate: 0.0000e+00\n",
      "Epoch 408/500\n",
      " - learning_rate: 9.121100e-05\n",
      "6/6 - 0s - 18ms/step - loss: 209395920.0000 - mae: 209395920.0000 - rmse: 786311680.0000 - success_rate: 0.0000e+00\n",
      "Epoch 409/500\n",
      " - learning_rate: 8.950377e-05\n",
      "6/6 - 0s - 19ms/step - loss: 198704208.0000 - mae: 198704208.0000 - rmse: 665782208.0000 - success_rate: 0.0000e+00\n",
      "Epoch 410/500\n",
      " - learning_rate: 8.781305e-05\n",
      "6/6 - 0s - 16ms/step - loss: 202581568.0000 - mae: 202581568.0000 - rmse: 734333888.0000 - success_rate: 0.0000e+00\n",
      "Epoch 411/500\n",
      " - learning_rate: 8.613901e-05\n",
      "6/6 - 0s - 17ms/step - loss: 201368848.0000 - mae: 201368848.0000 - rmse: 651650368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 412/500\n",
      " - learning_rate: 8.448167e-05\n",
      "6/6 - 0s - 16ms/step - loss: 202414560.0000 - mae: 202414560.0000 - rmse: 620144448.0000 - success_rate: 0.0000e+00\n",
      "Epoch 413/500\n",
      " - learning_rate: 8.284107e-05\n",
      "6/6 - 0s - 16ms/step - loss: 207255824.0000 - mae: 207255824.0000 - rmse: 944543488.0000 - success_rate: 0.0000e+00\n",
      "Epoch 414/500\n",
      " - learning_rate: 8.121734e-05\n",
      "6/6 - 0s - 16ms/step - loss: 189449184.0000 - mae: 189449184.0000 - rmse: 564223744.0000 - success_rate: 0.0000e+00\n",
      "Epoch 415/500\n",
      " - learning_rate: 7.961047e-05\n",
      "6/6 - 0s - 16ms/step - loss: 207590720.0000 - mae: 207590720.0000 - rmse: 770348672.0000 - success_rate: 0.0000e+00\n",
      "Epoch 416/500\n",
      " - learning_rate: 7.802060e-05\n",
      "6/6 - 0s - 16ms/step - loss: 193906704.0000 - mae: 193906704.0000 - rmse: 620218816.0000 - success_rate: 0.0000e+00\n",
      "Epoch 417/500\n",
      " - learning_rate: 7.644773e-05\n",
      "6/6 - 0s - 18ms/step - loss: 220111440.0000 - mae: 220111440.0000 - rmse: 977866560.0000 - success_rate: 0.0000e+00\n",
      "Epoch 418/500\n",
      " - learning_rate: 7.489197e-05\n",
      "6/6 - 0s - 18ms/step - loss: 199187584.0000 - mae: 199187584.0000 - rmse: 622280192.0000 - success_rate: 0.0000e+00\n",
      "Epoch 419/500\n",
      " - learning_rate: 7.335335e-05\n",
      "6/6 - 0s - 18ms/step - loss: 206984400.0000 - mae: 206984400.0000 - rmse: 813537216.0000 - success_rate: 0.0000e+00\n",
      "Epoch 420/500\n",
      " - learning_rate: 7.183194e-05\n",
      "6/6 - 0s - 16ms/step - loss: 193394864.0000 - mae: 193394864.0000 - rmse: 620858368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 421/500\n",
      " - learning_rate: 7.032780e-05\n",
      "6/6 - 0s - 16ms/step - loss: 199177872.0000 - mae: 199177872.0000 - rmse: 725345664.0000 - success_rate: 0.0000e+00\n",
      "Epoch 422/500\n",
      " - learning_rate: 6.884099e-05\n",
      "6/6 - 0s - 23ms/step - loss: 204531760.0000 - mae: 204531760.0000 - rmse: 920448448.0000 - success_rate: 0.0000e+00\n",
      "Epoch 423/500\n",
      " - learning_rate: 6.737158e-05\n",
      "6/6 - 0s - 16ms/step - loss: 204566704.0000 - mae: 204566704.0000 - rmse: 985568320.0000 - success_rate: 0.0000e+00\n",
      "Epoch 424/500\n",
      " - learning_rate: 6.591962e-05\n",
      "6/6 - 0s - 23ms/step - loss: 197687280.0000 - mae: 197687280.0000 - rmse: 741933888.0000 - success_rate: 0.0000e+00\n",
      "Epoch 425/500\n",
      " - learning_rate: 6.448518e-05\n",
      "6/6 - 0s - 23ms/step - loss: 192181376.0000 - mae: 192181376.0000 - rmse: 723641408.0000 - success_rate: 0.0000e+00\n",
      "Epoch 426/500\n",
      " - learning_rate: 6.306823e-05\n",
      "6/6 - 0s - 16ms/step - loss: 193917232.0000 - mae: 193917232.0000 - rmse: 710973120.0000 - success_rate: 0.0000e+00\n",
      "Epoch 427/500\n",
      " - learning_rate: 6.166896e-05\n",
      "6/6 - 0s - 20ms/step - loss: 199324352.0000 - mae: 199324352.0000 - rmse: 593191552.0000 - success_rate: 0.0000e+00\n",
      "Epoch 428/500\n",
      " - learning_rate: 6.028740e-05\n",
      "6/6 - 0s - 17ms/step - loss: 205881104.0000 - mae: 205881104.0000 - rmse: 736220416.0000 - success_rate: 0.0000e+00\n",
      "Epoch 429/500\n",
      " - learning_rate: 5.892356e-05\n",
      "6/6 - 0s - 16ms/step - loss: 223987296.0000 - mae: 223987296.0000 - rmse: 907126144.0000 - success_rate: 0.0000e+00\n",
      "Epoch 430/500\n",
      " - learning_rate: 5.757749e-05\n",
      "6/6 - 0s - 17ms/step - loss: 201752240.0000 - mae: 201752240.0000 - rmse: 954598656.0000 - success_rate: 0.0000e+00\n",
      "Epoch 431/500\n",
      " - learning_rate: 5.624926e-05\n",
      "6/6 - 0s - 16ms/step - loss: 200895696.0000 - mae: 200895696.0000 - rmse: 677543872.0000 - success_rate: 0.0000e+00\n",
      "Epoch 432/500\n",
      " - learning_rate: 5.493894e-05\n",
      "6/6 - 0s - 17ms/step - loss: 199119104.0000 - mae: 199119104.0000 - rmse: 694896384.0000 - success_rate: 0.0000e+00\n",
      "Epoch 433/500\n",
      " - learning_rate: 5.364657e-05\n",
      "6/6 - 0s - 16ms/step - loss: 196020080.0000 - mae: 196020080.0000 - rmse: 722154496.0000 - success_rate: 0.0000e+00\n",
      "Epoch 434/500\n",
      " - learning_rate: 5.237219e-05\n",
      "6/6 - 0s - 23ms/step - loss: 213665440.0000 - mae: 213665440.0000 - rmse: 1021094784.0000 - success_rate: 0.0000e+00\n",
      "Epoch 435/500\n",
      " - learning_rate: 5.111584e-05\n",
      "6/6 - 0s - 17ms/step - loss: 195879184.0000 - mae: 195879184.0000 - rmse: 647876288.0000 - success_rate: 0.0000e+00\n",
      "Epoch 436/500\n",
      " - learning_rate: 4.987760e-05\n",
      "6/6 - 0s - 24ms/step - loss: 189470240.0000 - mae: 189470240.0000 - rmse: 853422528.0000 - success_rate: 0.0000e+00\n",
      "Epoch 437/500\n",
      " - learning_rate: 4.865754e-05\n",
      "6/6 - 0s - 27ms/step - loss: 187938624.0000 - mae: 187938624.0000 - rmse: 499475904.0000 - success_rate: 0.0000e+00\n",
      "Epoch 438/500\n",
      " - learning_rate: 4.745565e-05\n",
      "6/6 - 0s - 26ms/step - loss: 203825952.0000 - mae: 203825952.0000 - rmse: 781809600.0000 - success_rate: 0.0000e+00\n",
      "Epoch 439/500\n",
      " - learning_rate: 4.627200e-05\n",
      "6/6 - 0s - 49ms/step - loss: 195913744.0000 - mae: 195913744.0000 - rmse: 803052992.0000 - success_rate: 0.0000e+00\n",
      "Epoch 440/500\n",
      " - learning_rate: 4.510664e-05\n",
      "6/6 - 0s - 28ms/step - loss: 204413232.0000 - mae: 204413232.0000 - rmse: 794940096.0000 - success_rate: 0.0000e+00\n",
      "Epoch 441/500\n",
      " - learning_rate: 4.395963e-05\n",
      "6/6 - 0s - 26ms/step - loss: 196101824.0000 - mae: 196101824.0000 - rmse: 657341568.0000 - success_rate: 0.0000e+00\n",
      "Epoch 442/500\n",
      " - learning_rate: 4.283100e-05\n",
      "6/6 - 0s - 49ms/step - loss: 199126368.0000 - mae: 199126368.0000 - rmse: 710222784.0000 - success_rate: 0.0000e+00\n",
      "Epoch 443/500\n",
      " - learning_rate: 4.172081e-05\n",
      "6/6 - 0s - 26ms/step - loss: 197054048.0000 - mae: 197054048.0000 - rmse: 762979904.0000 - success_rate: 0.0000e+00\n",
      "Epoch 444/500\n",
      " - learning_rate: 4.062909e-05\n",
      "6/6 - 0s - 26ms/step - loss: 193618928.0000 - mae: 193618928.0000 - rmse: 640906560.0000 - success_rate: 0.0000e+00\n",
      "Epoch 445/500\n",
      " - learning_rate: 3.955587e-05\n",
      "6/6 - 0s - 26ms/step - loss: 209640176.0000 - mae: 209640176.0000 - rmse: 740521600.0000 - success_rate: 0.0000e+00\n",
      "Epoch 446/500\n",
      " - learning_rate: 3.850125e-05\n",
      "6/6 - 0s - 27ms/step - loss: 188957712.0000 - mae: 188957712.0000 - rmse: 520298048.0000 - success_rate: 0.0000e+00\n",
      "Epoch 447/500\n",
      " - learning_rate: 3.746520e-05\n",
      "6/6 - 0s - 51ms/step - loss: 189744096.0000 - mae: 189744096.0000 - rmse: 572818048.0000 - success_rate: 0.0000e+00\n",
      "Epoch 448/500\n",
      " - learning_rate: 3.644780e-05\n",
      "6/6 - 0s - 50ms/step - loss: 206378064.0000 - mae: 206378064.0000 - rmse: 713541056.0000 - success_rate: 0.0000e+00\n",
      "Epoch 449/500\n",
      " - learning_rate: 3.544908e-05\n",
      "6/6 - 0s - 29ms/step - loss: 210746816.0000 - mae: 210746816.0000 - rmse: 688634752.0000 - success_rate: 0.0000e+00\n",
      "Epoch 450/500\n",
      " - learning_rate: 3.446907e-05\n",
      "6/6 - 0s - 44ms/step - loss: 174739536.0000 - mae: 174739536.0000 - rmse: 500892032.0000 - success_rate: 0.0000e+00\n",
      "Epoch 451/500\n",
      " - learning_rate: 3.350785e-05\n",
      "6/6 - 0s - 31ms/step - loss: 190976096.0000 - mae: 190976096.0000 - rmse: 603367040.0000 - success_rate: 0.0000e+00\n",
      "Epoch 452/500\n",
      " - learning_rate: 3.256542e-05\n",
      "6/6 - 0s - 46ms/step - loss: 198302512.0000 - mae: 198302512.0000 - rmse: 816208384.0000 - success_rate: 0.0000e+00\n",
      "Epoch 453/500\n",
      " - learning_rate: 3.164182e-05\n",
      "6/6 - 0s - 20ms/step - loss: 203715328.0000 - mae: 203715328.0000 - rmse: 653238720.0000 - success_rate: 0.0000e+00\n",
      "Epoch 454/500\n",
      " - learning_rate: 3.073713e-05\n",
      "6/6 - 0s - 16ms/step - loss: 195922112.0000 - mae: 195922112.0000 - rmse: 883497280.0000 - success_rate: 0.0000e+00\n",
      "Epoch 455/500\n",
      " - learning_rate: 2.985132e-05\n",
      "6/6 - 0s - 23ms/step - loss: 188134256.0000 - mae: 188134256.0000 - rmse: 539349632.0000 - success_rate: 0.0000e+00\n",
      "Epoch 456/500\n",
      " - learning_rate: 2.898445e-05\n",
      "6/6 - 0s - 16ms/step - loss: 198703792.0000 - mae: 198703792.0000 - rmse: 705340480.0000 - success_rate: 0.0000e+00\n",
      "Epoch 457/500\n",
      " - learning_rate: 2.813659e-05\n",
      "6/6 - 0s - 16ms/step - loss: 206796944.0000 - mae: 206796944.0000 - rmse: 886114432.0000 - success_rate: 0.0000e+00\n",
      "Epoch 458/500\n",
      " - learning_rate: 2.730772e-05\n",
      "6/6 - 0s - 16ms/step - loss: 219079136.0000 - mae: 219079136.0000 - rmse: 1159524864.0000 - success_rate: 0.0000e+00\n",
      "Epoch 459/500\n",
      " - learning_rate: 2.649792e-05\n",
      "6/6 - 0s - 23ms/step - loss: 191445056.0000 - mae: 191445056.0000 - rmse: 596296064.0000 - success_rate: 0.0000e+00\n",
      "Epoch 460/500\n",
      " - learning_rate: 2.570718e-05\n",
      "6/6 - 0s - 16ms/step - loss: 208150112.0000 - mae: 208150112.0000 - rmse: 810717568.0000 - success_rate: 0.0000e+00\n",
      "Epoch 461/500\n",
      " - learning_rate: 2.493558e-05\n",
      "6/6 - 0s - 16ms/step - loss: 192054048.0000 - mae: 192054048.0000 - rmse: 718675392.0000 - success_rate: 0.0000e+00\n",
      "Epoch 462/500\n",
      " - learning_rate: 2.418310e-05\n",
      "6/6 - 0s - 16ms/step - loss: 213026480.0000 - mae: 213026480.0000 - rmse: 957674368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 463/500\n",
      " - learning_rate: 2.344980e-05\n",
      "6/6 - 0s - 16ms/step - loss: 207741296.0000 - mae: 207741296.0000 - rmse: 851127616.0000 - success_rate: 0.0000e+00\n",
      "Epoch 464/500\n",
      " - learning_rate: 2.273568e-05\n",
      "6/6 - 0s - 18ms/step - loss: 197380816.0000 - mae: 197380816.0000 - rmse: 646872448.0000 - success_rate: 0.0000e+00\n",
      "Epoch 465/500\n",
      " - learning_rate: 2.204082e-05\n",
      "6/6 - 0s - 17ms/step - loss: 182761072.0000 - mae: 182761072.0000 - rmse: 590976704.0000 - success_rate: 0.0000e+00\n",
      "Epoch 466/500\n",
      " - learning_rate: 2.136521e-05\n",
      "6/6 - 0s - 17ms/step - loss: 212865504.0000 - mae: 212865504.0000 - rmse: 1211318912.0000 - success_rate: 0.0000e+00\n",
      "Epoch 467/500\n",
      " - learning_rate: 2.070888e-05\n",
      "6/6 - 0s - 16ms/step - loss: 196798160.0000 - mae: 196798160.0000 - rmse: 655397952.0000 - success_rate: 0.0000e+00\n",
      "Epoch 468/500\n",
      " - learning_rate: 2.007188e-05\n",
      "6/6 - 0s - 31ms/step - loss: 174349984.0000 - mae: 174349984.0000 - rmse: 478769600.0000 - success_rate: 0.0000e+00\n",
      "Epoch 469/500\n",
      " - learning_rate: 1.945418e-05\n",
      "6/6 - 0s - 17ms/step - loss: 200449824.0000 - mae: 200449824.0000 - rmse: 639470272.0000 - success_rate: 0.0000e+00\n",
      "Epoch 470/500\n",
      " - learning_rate: 1.885586e-05\n",
      "6/6 - 0s - 16ms/step - loss: 197885456.0000 - mae: 197885456.0000 - rmse: 657832384.0000 - success_rate: 0.0000e+00\n",
      "Epoch 471/500\n",
      " - learning_rate: 1.827693e-05\n",
      "6/6 - 0s - 16ms/step - loss: 188811904.0000 - mae: 188811904.0000 - rmse: 592496832.0000 - success_rate: 0.0000e+00\n",
      "Epoch 472/500\n",
      " - learning_rate: 1.771741e-05\n",
      "6/6 - 0s - 16ms/step - loss: 187929648.0000 - mae: 187929648.0000 - rmse: 661652480.0000 - success_rate: 0.0000e+00\n",
      "Epoch 473/500\n",
      " - learning_rate: 1.717733e-05\n",
      "6/6 - 0s - 17ms/step - loss: 178067808.0000 - mae: 178067808.0000 - rmse: 554128960.0000 - success_rate: 0.0000e+00\n",
      "Epoch 474/500\n",
      " - learning_rate: 1.665670e-05\n",
      "6/6 - 0s - 16ms/step - loss: 208419568.0000 - mae: 208419568.0000 - rmse: 835815936.0000 - success_rate: 0.0000e+00\n",
      "Epoch 475/500\n",
      " - learning_rate: 1.615554e-05\n",
      "6/6 - 0s - 16ms/step - loss: 180370704.0000 - mae: 180370704.0000 - rmse: 534804704.0000 - success_rate: 0.0000e+00\n",
      "Epoch 476/500\n",
      " - learning_rate: 1.567385e-05\n",
      "6/6 - 0s - 16ms/step - loss: 218654672.0000 - mae: 218654672.0000 - rmse: 845885376.0000 - success_rate: 0.0000e+00\n",
      "Epoch 477/500\n",
      " - learning_rate: 1.521170e-05\n",
      "6/6 - 0s - 18ms/step - loss: 196487376.0000 - mae: 196487376.0000 - rmse: 997772352.0000 - success_rate: 0.0000e+00\n",
      "Epoch 478/500\n",
      " - learning_rate: 1.476908e-05\n",
      "6/6 - 0s - 28ms/step - loss: 172783664.0000 - mae: 172783664.0000 - rmse: 422191328.0000 - success_rate: 0.0000e+00\n",
      "Epoch 479/500\n",
      " - learning_rate: 1.434601e-05\n",
      "6/6 - 0s - 17ms/step - loss: 193518608.0000 - mae: 193518624.0000 - rmse: 911804096.0000 - success_rate: 0.0000e+00\n",
      "Epoch 480/500\n",
      " - learning_rate: 1.394248e-05\n",
      "6/6 - 0s - 17ms/step - loss: 226764080.0000 - mae: 226764080.0000 - rmse: 1360308864.0000 - success_rate: 0.0000e+00\n",
      "Epoch 481/500\n",
      " - learning_rate: 1.355854e-05\n",
      "6/6 - 0s - 16ms/step - loss: 180649264.0000 - mae: 180649264.0000 - rmse: 549451392.0000 - success_rate: 0.0000e+00\n",
      "Epoch 482/500\n",
      " - learning_rate: 1.319422e-05\n",
      "6/6 - 0s - 18ms/step - loss: 192824608.0000 - mae: 192824608.0000 - rmse: 630835840.0000 - success_rate: 0.0000e+00\n",
      "Epoch 483/500\n",
      " - learning_rate: 1.284950e-05\n",
      "6/6 - 0s - 16ms/step - loss: 214992720.0000 - mae: 214992720.0000 - rmse: 862611520.0000 - success_rate: 0.0000e+00\n",
      "Epoch 484/500\n",
      " - learning_rate: 1.252442e-05\n",
      "6/6 - 0s - 16ms/step - loss: 227402144.0000 - mae: 227402160.0000 - rmse: 1138974464.0000 - success_rate: 0.0000e+00\n",
      "Epoch 485/500\n",
      " - learning_rate: 1.221896e-05\n",
      "6/6 - 0s - 22ms/step - loss: 202119808.0000 - mae: 202119808.0000 - rmse: 785002816.0000 - success_rate: 0.0000e+00\n",
      "Epoch 486/500\n",
      " - learning_rate: 1.193315e-05\n",
      "6/6 - 0s - 20ms/step - loss: 191606304.0000 - mae: 191606304.0000 - rmse: 670657280.0000 - success_rate: 0.0000e+00\n",
      "Epoch 487/500\n",
      " - learning_rate: 1.166699e-05\n",
      "6/6 - 0s - 17ms/step - loss: 206039248.0000 - mae: 206039248.0000 - rmse: 721213760.0000 - success_rate: 0.0000e+00\n",
      "Epoch 488/500\n",
      " - learning_rate: 1.142051e-05\n",
      "6/6 - 0s - 16ms/step - loss: 192347824.0000 - mae: 192347824.0000 - rmse: 623707648.0000 - success_rate: 0.0000e+00\n",
      "Epoch 489/500\n",
      " - learning_rate: 1.119371e-05\n",
      "6/6 - 0s - 17ms/step - loss: 215362784.0000 - mae: 215362784.0000 - rmse: 1035458368.0000 - success_rate: 0.0000e+00\n",
      "Epoch 490/500\n",
      " - learning_rate: 1.098659e-05\n",
      "6/6 - 0s - 17ms/step - loss: 191648656.0000 - mae: 191648656.0000 - rmse: 632773888.0000 - success_rate: 0.0000e+00\n",
      "Epoch 491/500\n",
      " - learning_rate: 1.079921e-05\n",
      "6/6 - 0s - 17ms/step - loss: 215359152.0000 - mae: 215359152.0000 - rmse: 1001849088.0000 - success_rate: 0.0000e+00\n",
      "Epoch 492/500\n",
      " - learning_rate: 1.063151e-05\n",
      "6/6 - 0s - 16ms/step - loss: 188696800.0000 - mae: 188696800.0000 - rmse: 611088768.0000 - success_rate: 0.0000e+00\n",
      "Epoch 493/500\n",
      " - learning_rate: 1.048352e-05\n",
      "6/6 - 0s - 16ms/step - loss: 208078272.0000 - mae: 208078272.0000 - rmse: 882436352.0000 - success_rate: 0.0000e+00\n",
      "Epoch 494/500\n",
      " - learning_rate: 1.035526e-05\n",
      "6/6 - 0s - 23ms/step - loss: 190842208.0000 - mae: 190842208.0000 - rmse: 498914112.0000 - success_rate: 0.0000e+00\n",
      "Epoch 495/500\n",
      " - learning_rate: 1.024671e-05\n",
      "6/6 - 0s - 17ms/step - loss: 205972784.0000 - mae: 205972784.0000 - rmse: 828356480.0000 - success_rate: 0.0000e+00\n",
      "Epoch 496/500\n",
      " - learning_rate: 1.015791e-05\n",
      "6/6 - 0s - 22ms/step - loss: 193694128.0000 - mae: 193694128.0000 - rmse: 655525760.0000 - success_rate: 0.0000e+00\n",
      "Epoch 497/500\n",
      " - learning_rate: 1.008881e-05\n",
      "6/6 - 0s - 23ms/step - loss: 194227344.0000 - mae: 194227344.0000 - rmse: 700777920.0000 - success_rate: 0.0000e+00\n",
      "Epoch 498/500\n",
      " - learning_rate: 1.003948e-05\n",
      "6/6 - 0s - 24ms/step - loss: 205419200.0000 - mae: 205419200.0000 - rmse: 700916160.0000 - success_rate: 0.0000e+00\n",
      "Epoch 499/500\n",
      " - learning_rate: 1.000985e-05\n",
      "6/6 - 0s - 17ms/step - loss: 179869328.0000 - mae: 179869328.0000 - rmse: 478284416.0000 - success_rate: 0.0000e+00\n",
      "Epoch 500/500\n",
      " - learning_rate: 1.000000e-05\n",
      "6/6 - 0s - 16ms/step - loss: 205681744.0000 - mae: 205681744.0000 - rmse: 853898112.0000 - success_rate: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. Training\n",
    "# ============================================================\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1760403610761,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "o16AHGLtWk-A",
    "outputId": "4b69289a-9ea9-4b76-8e5d-b04f2429852d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - 172ms/step - loss: 362688384.0000 - mae: 362688384.0000 - rmse: 1400402688.0000 - success_rate: 0.0000e+00\n",
      "\n",
      "Test metrics (Smooth L1 / Huber + SuccessRate):\n",
      "  loss: 362688384.000000\n",
      "  mae: 362688384.000000\n",
      "  rmse: 1400402688.000000\n",
      "  success_rate: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. Evaluation\n",
    "# ============================================================\n",
    "eval_results = model.evaluate(test_ds, return_dict=True, verbose=2)\n",
    "\n",
    "print(\"\\nTest metrics (Smooth L1 / Huber + SuccessRate):\")\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"  {k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1760403611190,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "-dWT-bM7XNAF",
    "outputId": "ee012a2c-6aab-485a-abd6-1e6a673d3b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Predictions (25 rows) ===\n",
      " feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  feature_18  feature_19  feature_20  feature_21  feature_22  feature_23  feature_24  feature_25  feature_26  feature_27  feature_28  feature_29  feature_30  feature_31  feature_32  feature_33  feature_34  feature_35  feature_36  feature_37  feature_38  feature_39  feature_40  feature_41  feature_42  feature_43  feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  feature_50  feature_51  feature_52  feature_53  feature_54  feature_55  feature_56  feature_57  feature_58  feature_59  feature_60  feature_61  feature_62  feature_63  feature_64  feature_65  feature_66  feature_67  feature_68  feature_69  feature_70  feature_71  feature_72  feature_73  feature_74  feature_75  feature_76  feature_77  feature_78  feature_79  feature_80  feature_81  feature_82  feature_83  feature_84  feature_85  feature_86        y_true        y_pred  pct_error  abs_pct_error\n",
      "  0.196839   0.139718   0.209781   0.236502   0.061305  -0.004041   0.079267   0.078683   0.139441  -0.269895    0.178780   -0.255996   -0.068121   -0.088721   -0.067912   -0.080342   -0.226908   -0.224748   -0.240834   -0.218224   -0.173713   -0.167637   -0.209912   -0.189847    0.111691    0.089300    0.249697   -0.138811    0.010498   -0.020281   -0.038537   -0.194456    0.140098    0.135412    0.125217    0.125382    0.124263   -0.257333   -0.293794   -0.300592   -0.297559   -0.205623   -0.058552   -0.052854    0.029103   -0.219183   -0.007992   -0.032898   -0.009992   -0.000136   -0.187305   -0.191468   -0.209177   -0.212259   -0.162693   -0.168490   -0.154237   -0.160047    0.096297    0.093572    0.105477    0.101316   -0.223991   -0.259807   -0.259825   -0.261501   -0.033972   -0.044284   -0.038894   -0.051251    0.112338    0.110725    0.123551    0.120118   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  1.267140e+09  1.299177e+09   2.528312       2.528312\n",
      "  0.253676   0.146312   0.207710   0.255235  -0.064773  -0.098907  -0.116070  -0.100409  -0.371133  -0.257985   -0.347680   -0.176413   -0.277646   -0.263983   -0.266962   -0.259515   -0.219979   -0.234353   -0.253593   -0.254674   -0.255378   -0.263348   -0.273835   -0.274507   -0.362954   -0.448757   -0.373795   -0.482131   -0.228993   -0.186535   -0.094896   -0.232957   -0.130646   -0.133475   -0.139203   -0.143886    0.124263   -0.259618   -0.264639   -0.270557   -0.270214   -0.271269   -0.492978   -0.387236   -0.453607   -0.428524   -0.519694   -0.512190   -0.562960   -0.513839   -0.215765   -0.200772   -0.197204   -0.197287   -0.318838   -0.291000   -0.293941   -0.283672   -0.199396   -0.205768   -0.208380   -0.205171   -0.263349   -0.270898   -0.270913   -0.272739   -0.362664   -0.376774   -0.384856   -0.391320   -0.167433   -0.172156   -0.173260   -0.169776   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402    2.583065   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  2.320880e+08  2.507727e+08   8.050712       8.050712\n",
      "  0.223707   0.131318   0.197938   0.215082  -0.137242  -0.142675  -0.147964  -0.142936  -0.256213  -0.203630   -0.260545   -0.117692   -0.266966   -0.255543   -0.258271   -0.251812   -0.265020   -0.272258   -0.283688   -0.271442   -0.254578   -0.260026   -0.270920   -0.267156   -0.027538    0.019013    0.022404    0.012132   -0.180835   -0.129476   -0.043725   -0.166625   -0.129152   -0.132225   -0.138197   -0.143203    0.124263   -0.325158   -0.328466   -0.337233   -0.332925   -0.375173   -0.269774   -0.189704   -0.238744   -0.234448   -0.320443   -0.291815   -0.303176   -0.307518   -0.329197   -0.307038   -0.314500   -0.312345   -0.312384   -0.285356   -0.289340   -0.281425   -0.204843   -0.210110   -0.212212   -0.206788   -0.329534   -0.335250   -0.336312   -0.335396   -0.327575   -0.336914   -0.339085   -0.336340   -0.178726   -0.182979   -0.184554   -0.179731   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402   -0.387137    2.248781   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  2.555460e+08  2.612434e+08   2.229498       2.229498\n",
      "  0.365903   0.147584   0.229890   0.264413  -0.112780  -0.083333  -0.118880  -0.120430  -0.165721  -0.200925   -0.174263   -0.134036   -0.244714   -0.235771   -0.237688   -0.231700   -0.253705   -0.253933   -0.271356   -0.263543   -0.244532   -0.252414   -0.268252   -0.269090   -0.033867   -0.039155    0.003637   -0.023384   -0.154586   -0.124453   -0.034620   -0.151594    0.151015    0.142963    0.125843    0.121577    0.124263   -0.074561   -0.069119   -0.111245   -0.147462   -0.350758   -0.239444   -0.174738   -0.208441   -0.211434   -0.330520   -0.326775   -0.340213   -0.335049   -0.180548   -0.187590   -0.208884   -0.213975   -0.307736   -0.283105   -0.286307   -0.277470    0.076845    0.068360    0.067450    0.057731   -0.116900   -0.114659   -0.148166   -0.178142   -0.198254   -0.207018   -0.209455   -0.206371    0.115222    0.107160    0.107324    0.094738   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  2.739290e+08  2.416586e+08 -11.780560      11.780560\n",
      "  0.263596   0.149264   0.233171   0.258551  -0.136787  -0.133811  -0.138657  -0.132736  -0.281312  -0.223584   -0.297140   -0.136953   -0.284960   -0.265712   -0.269559   -0.261826   -0.280632   -0.286278   -0.300514   -0.291804   -0.257131   -0.262933   -0.274819   -0.279763   -0.031757   -0.334844   -0.038069    0.021011   -0.221383   -0.192324   -0.087720   -0.216070   -0.112500   -0.100588   -0.105739   -0.111024    0.124263   -0.324696   -0.324643   -0.332345   -0.329142   -0.388270   -0.314023   -0.261838   -0.284065   -0.268555   -0.372975   -0.346383   -0.356278   -0.351810   -0.343642   -0.312688   -0.317369   -0.318873   -0.340792   -0.306441   -0.311099   -0.300841   -0.186413   -0.174295   -0.176054   -0.173004   -0.329265   -0.332112   -0.332328   -0.332248   -0.345621   -0.342662   -0.345884   -0.345281   -0.155538   -0.142286   -0.143203   -0.140886   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  6.181200e+07  8.482458e+07  37.229950      37.229950\n",
      "  0.053055   0.057976  -0.034803  -0.078085  -0.117363  -0.066783  -0.080110  -0.100846   0.657334  -0.209776    0.357309    0.315320    0.510640    0.843363    0.483931    0.738961    0.276027    0.177341    0.227160    0.262240    0.314248    0.211909    0.348246    0.371093    0.525160   -1.154047   -0.046410    0.814199    0.993910   -0.893133    0.092136    0.663349    0.047544    0.094424    0.002980    0.061399    0.124263    0.348115    0.316605    0.288301    0.270806    0.112042    0.870070   -1.215150    0.571617    0.765710    1.484766   -1.579001    0.895180    1.012094    0.259438    0.219653    0.191374    0.165254    0.667304    0.556073    0.512428    0.717269   -0.002109   -0.028182   -0.024229   -0.019528    0.265584    0.234624    0.243799    0.233795   -0.142030   -0.224054   -0.229425   -0.213455    0.019033    0.002362    0.008184    0.009990   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294    5.866026   -0.879082    0.879082  6.740000e+09  6.662235e+09  -1.153778       1.153778\n",
      " -1.717567  -1.177759  -1.197434  -1.034356  -0.111131  -0.116497  -0.120396  -0.119419   0.083015   0.043382   -0.004742    0.065269   -0.203632   -0.199263   -0.200526   -0.192880   -0.268573   -0.276801   -0.288789   -0.281618   -0.195998   -0.196619   -0.188067   -0.135232   -0.014881   -0.036732   -0.013046    0.710611   -0.132904   -0.111504   -0.025844    0.131912   -0.116765   -0.118745   -0.123624   -0.127176    0.124263    0.953906    0.926646    0.958585    0.842606   -0.284396   -0.159647   -0.121341   -0.149133    0.237000   -0.202405   -0.200212   -0.195167   -0.193203   -0.318447   -0.301086   -0.302261   -0.290218   -0.244280   -0.230924   -0.231661   -0.219804    0.040827    0.039295    0.043231    0.038340    0.749036    0.726227    0.735269    0.699786    0.082233    0.081120    0.070709    0.085447    0.033313    0.031507    0.037345    0.029893   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402   -0.387137    2.248781   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  7.316960e+08  8.366762e+08  14.347519      14.347519\n",
      " -1.238186  -0.508358  -1.131759  -0.975019  -0.091596  -0.120268  -0.150567  -0.118760   0.324648  -0.049390   -0.023443    0.409628   -0.115272   -0.120318   -0.104921   -0.055044   -0.121254   -0.152046   -0.148135   -0.147514   -0.091782   -0.130843   -0.035112   -0.103667   -0.048633   -0.031884   -0.004704    0.044583    0.167906    0.096557    0.233819    0.384578    0.058520    0.056862    0.069217    0.091773    0.124263    0.642171    0.637664    0.635240    0.665015   -0.027625   -0.005520    0.065817    0.057650    0.135499   -0.016001    0.064679    0.160832    0.218877   -0.100580   -0.124874   -0.087342   -0.020237   -0.101312   -0.104091   -0.076155   -0.015745    0.068081    0.060613    0.072992    0.069216    0.533118    0.514598    0.541006    0.546727    0.157333    0.138258    0.142876    0.147588    0.052414    0.046620    0.059451    0.054960   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294    5.866026   -0.879082    0.879082  1.838000e+09  1.732836e+09  -5.721663       5.721663\n",
      "  0.248887   0.145274   0.227324   0.250457  -0.110613  -0.110489  -0.134203  -0.127573  -0.180432  -0.152938   -0.236957   -0.077207   -0.142050   -0.155888   -0.162419   -0.146104   -0.192026   -0.201859   -0.219578   -0.210520   -0.234315   -0.244674   -0.259068   -0.256832    0.371164    0.336515    0.312254    0.506394   -0.083620   -0.117923   -0.019974   -0.129113   -0.112650   -0.109485   -0.113116   -0.143808    0.124263   -0.296864   -0.300316   -0.307317   -0.304063   -0.216701   -0.158763   -0.136296   -0.175825   -0.153043   -0.204672   -0.237648   -0.238697   -0.222658   -0.207955   -0.213122   -0.220615   -0.205221   -0.169216   -0.181679   -0.187779   -0.168635   -0.188429   -0.194045   -0.195640   -0.191264   -0.304411   -0.310212   -0.311630   -0.311154   -0.259768   -0.269509   -0.270069   -0.269234   -0.170592   -0.175251   -0.176668   -0.172451   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402   -0.387137    2.248781   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  1.224730e+09  1.175656e+09  -4.006917       4.006917\n",
      "  0.263699   0.247650   0.432867   0.836948  -0.137262  -0.147176  -0.153427  -0.151437  -0.262845  -0.200304   -0.240213   -0.088444   -0.299744   -0.265232   -0.286190   -0.266445   -0.246177   -0.237303   -0.278671   -0.265880   -0.244532   -0.252414   -0.268252   -0.269090   -0.407254    0.057792   -0.071433   -0.159528   -0.417894    0.001821   -0.218622   -0.129690    0.003595    0.000219    0.022526    0.027912    0.124263   -0.305060   -0.307160   -0.313986   -0.305867   -0.375824   -0.404968   -0.169178   -0.365816   -0.255441   -0.330520   -0.326775   -0.340213   -0.335049   -0.271789   -0.260909   -0.257283   -0.256216   -0.352934   -0.306067   -0.324037   -0.304424   -0.130430   -0.110786   -0.109815   -0.094063   -0.301838   -0.307178   -0.307920   -0.309144   -0.291877   -0.295270   -0.295726   -0.276497   -0.101304   -0.078788   -0.077055   -0.063428   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082 -7.231900e+07 -5.971686e+07  17.425760      17.425760\n",
      "  0.239742   0.135580   0.219854   0.245445  -0.121093  -0.125341  -0.123963  -0.097276  -0.248291  -0.198029   -0.248352   -0.129980   -0.271852   -0.261144   -0.263064   -0.256227   -0.234149   -0.241802   -0.249212   -0.240571   -0.248247   -0.254903   -0.261771   -0.237808   -0.122467   -0.090052    0.026574   -2.032991   -0.209145   -0.153655   -0.095229   -0.189553   -0.114307   -0.117128   -0.121630   -0.125542    0.124263   -0.294694   -0.298312   -0.308779   -0.305498   -0.350493   -0.300087   -0.210529   -0.226676   -0.756578   -0.334844   -0.330021   -0.348848   -0.320490   -0.236632   -0.225395   -0.234141   -0.235887   -0.297334   -0.275744   -0.279316   -0.270132   -0.192503   -0.197897   -0.200039   -0.195636   -0.299983   -0.305842   -0.306060   -0.305630   -0.286931   -0.295673   -0.298169   -0.315829   -0.171069   -0.175600   -0.177260   -0.170391   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402    2.583065   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  3.525240e+08  3.606607e+08   2.308137       2.308137\n",
      "  0.243647   0.143212   0.214774   0.248029  -0.132497  -0.132805  -0.139776  -0.130482  -0.263506  -0.088094   -0.179953   -0.144727   -0.275347   -0.262484   -0.265173   -0.259530   -0.293810   -0.299474   -0.312652   -0.303465   -0.244532   -0.252414   -0.268252   -0.269090    0.177087    0.162010    0.182968    0.195630   -0.219560   -0.180771   -0.089018   -0.232888   -0.061569   -0.063661   -0.069720   -0.073147    0.124263   -0.284113   -0.294973   -0.307611   -0.313048   -0.377115   -0.279363   -0.217019   -0.261575   -0.257610   -0.330520   -0.326775   -0.340213   -0.335049   -0.312120   -0.293720   -0.300496   -0.301791   -0.332896   -0.303925   -0.307687   -0.299060   -0.140072   -0.145334   -0.146765   -0.143869   -0.295040   -0.307018   -0.311873   -0.318711   -0.305571   -0.315409   -0.317341   -0.316507   -0.109982   -0.114178   -0.114930   -0.112800   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  8.179600e+07  1.309501e+08  60.093582      60.093582\n",
      "  0.261616   0.149354   0.233092   0.259291  -0.130279  -0.136904  -0.131730  -0.130451  -0.301138  -0.239033   -0.325403   -0.151921   -0.287010   -0.272179   -0.275269   -0.267169   -0.276364   -0.284739   -0.288440   -0.281319   -0.266540   -0.272898   -0.286328   -0.286864   -0.291230   -0.332420   -0.259106   -0.387422   -0.329356   -0.246614   -0.146248   -0.328530   -0.130223   -0.132945   -0.138826   -0.143778    0.124263   -0.330962   -0.332938   -0.340891   -0.336465   -0.368328   -0.368122   -0.278253   -0.328310   -0.329016   -0.422808   -0.399157   -0.423375   -0.412594   -0.315702   -0.297226   -0.298894   -0.293280   -0.337685   -0.307035   -0.310219   -0.299712   -0.209484   -0.215090   -0.216155   -0.211647   -0.332915   -0.337582   -0.337861   -0.336821   -0.347240   -0.358939   -0.352681   -0.353727   -0.180952   -0.185204   -0.186864   -0.182547   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402    2.583065   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  7.918200e+07  9.335450e+07  17.898634      17.898634\n",
      "  0.245568   0.143486   0.227501   0.250449  -0.131897  -0.135396  -0.140266  -0.137364  -0.268991  -0.210695   -0.284978   -0.127437   -0.264299   -0.252093   -0.253798   -0.244712   -0.280888   -0.287871   -0.300402   -0.291612   -0.259195   -0.266114   -0.278359   -0.278254   -0.186017   -0.233049   -0.181951   -0.239439   -0.204591   -0.171535   -0.076844   -0.214722   -0.129250   -0.131969   -0.123556   -0.142737    0.124263   -0.334214   -0.335960   -0.344037   -0.339417   -0.370698   -0.326732   -0.250418   -0.299918   -0.286546   -0.382072   -0.364745   -0.381565   -0.361713   -0.306603   -0.289017   -0.289614   -0.290228   -0.312274   -0.286105   -0.287562   -0.276634   -0.209758   -0.215182   -0.217418   -0.212686   -0.337250   -0.341572   -0.341892   -0.340786   -0.351723   -0.362042   -0.364788   -0.365112   -0.180588   -0.184840   -0.186415   -0.182011   -0.209379    2.772280   -0.236242   -0.224031   -0.464402   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  2.692170e+08  2.759084e+08   2.485489       2.485489\n",
      "  0.105216   0.057745  -0.040252   0.045166  -0.094875  -0.071915  -0.098920  -0.106283  -0.330350  -0.240678   -0.416332   -0.176259    0.320032    0.198725    0.114783    0.146444   -0.149450   -0.159751   -0.175982   -0.180275   -0.089195   -0.112499   -0.107915   -0.106164   -0.308106   -0.526314   -1.551964   -1.077022   -0.263798   -0.301894   -0.231111   -0.374596   -0.046185   -0.049600   -0.059887   -0.038454    0.124263   -0.173801   -0.162045   -0.166569   -0.151413   -0.363012   -0.386889   -0.348362   -0.865316   -0.538092   -0.352525   -0.506678   -0.675719   -0.531713   -0.367266   -0.282033   -0.294747   -0.302297    0.157495    0.052445   -0.029275    0.007531   -0.155825   -0.161810   -0.166051   -0.161695   -0.193369   -0.188294   -0.189500   -0.177325   -0.302742   -0.317257   -0.342410   -0.351363   -0.127922   -0.132222   -0.132584   -0.127424   -0.209379   -0.360714   -0.236242    4.463658   -0.464402   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  2.764600e+09  2.747941e+09  -0.602599       0.602599\n",
      " -1.411401  -0.555296  -1.025648  -0.840479   4.728133   4.619201   6.024837   4.624523   1.971087 -10.405138    7.880804  -12.990098    2.306686    1.940739    2.077386    2.271555   11.076760   11.494325   10.802466   11.079110    0.840316    1.106904    1.277809    1.397379    0.198182    0.220178    0.276805    0.515273    3.499194    2.659454    3.004506    4.245368   12.448572   12.732467   12.571752   14.276186    0.124263   17.769476   18.172581   18.271118   18.570145    2.650572    4.661053    3.751737    4.747262    5.803161    0.978996    0.897358    1.070556    1.211913    7.430603    6.278907    5.621445    6.169455    1.787822    1.413322    1.514604    1.664527    9.691123    9.851980    9.708117    9.992482   15.801693   16.415462   16.206621   16.805161    5.822289    5.822438    5.883548    5.958813    9.908190   10.080316    9.919922   10.212673   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  1.560400e+10  1.687414e+10   8.139835       8.139835\n",
      "  0.264769   0.149986   0.235615   0.263006  -0.125508  -0.117807  -0.127421  -0.122481  -0.230699  -0.182850   -0.256122   -0.109761   -0.261441   -0.251388   -0.254502   -0.246339   -0.253705   -0.253933   -0.271356   -0.263543   -0.244532   -0.252414   -0.268252   -0.269090   -0.042305   -0.031884    0.043256    0.023970   -0.213665   -0.166140   -0.064570   -0.203984   -0.013029   -0.016145   -0.023431   -0.028142    0.124263   -0.273690   -0.278150   -0.285156   -0.275269   -0.363858   -0.261833   -0.190141   -0.215176   -0.215866   -0.330520   -0.326775   -0.340213   -0.335049   -0.282077   -0.269314   -0.278812   -0.281495   -0.321474   -0.295277   -0.299387   -0.288826   -0.063033   -0.071219   -0.070887   -0.070959   -0.286250   -0.292789   -0.293302   -0.286935   -0.227887   -0.236936   -0.239698   -0.239584   -0.035905   -0.043560   -0.042254   -0.043248   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  1.643170e+08  1.804411e+08   9.812799       9.812799\n",
      "  0.262004   0.146629   0.232126   0.259867  -0.127868  -0.135287  -0.127666  -0.121421  -0.272729  -0.214195   -0.287890   -0.131205   -0.281123   -0.267177   -0.270097   -0.261784   -0.280632   -0.286278   -0.300514   -0.291804   -0.257131   -0.262933   -0.274819   -0.279763   -0.025428    0.074758    0.030745   -0.002667   -0.211658   -0.175219   -0.087856   -0.219519   -0.099903   -0.103116   -0.110313   -0.114018    0.124263   -0.311579   -0.311524   -0.326446   -0.318867   -0.392255   -0.307573   -0.226983   -0.279576   -0.272809   -0.372975   -0.346383   -0.356278   -0.351810   -0.335845   -0.313986   -0.319512   -0.320223   -0.337640   -0.307583   -0.311518   -0.300809   -0.169694   -0.175218   -0.176949   -0.172969   -0.318203   -0.321017   -0.327450   -0.323605   -0.334478   -0.343382   -0.341358   -0.340324   -0.138610   -0.143204   -0.144895   -0.141582   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  5.756200e+07  5.093990e+07 -11.504284      11.504284\n",
      "  0.250481   0.148158   0.233772   0.258883  -0.091708  -0.096285  -0.091295  -0.083459  -0.257821  -0.220644   -0.228938   -0.091007   -0.229637   -0.231879   -0.223411   -0.223913   -0.221375   -0.231098   -0.238251   -0.230800   -0.221369   -0.233914   -0.244198   -0.232819   -0.202629   -0.236049   -0.236168   -0.289753   -0.229121   -0.184332   -0.094938   -0.231010   -0.098537   -0.096177   -0.099745   -0.103630    0.124263   -0.236782   -0.242861   -0.248204   -0.259353   -0.350189   -0.376025   -0.253043   -0.427764   -0.361463   -0.460454   -0.320156   -0.431506   -0.378640   -0.105675   -0.186668   -0.145971   -0.152934   -0.247900   -0.243485   -0.236401   -0.233452   -0.194692   -0.200818   -0.203279   -0.199514   -0.222629   -0.233011   -0.234105   -0.236871   -0.394170   -0.401653   -0.406087   -0.405449   -0.157500   -0.162910   -0.164317   -0.161397   -0.209379    2.772280   -0.236242   -0.224031   -0.464402   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  6.069000e+08  6.065815e+08  -0.052474       0.052474\n",
      "  0.248034   0.140086   0.220363   0.241792  -0.045897  -0.052161  -0.065225  -0.051699  -0.173693  -0.175130   -0.129751    0.040768    0.105421    0.129617    0.123028    0.085124   -0.079799   -0.075105   -0.095265   -0.097914   -0.117596   -0.118564   -0.140479   -0.148970    0.075829    0.159586    0.164201    0.180832    0.007264    0.049326    0.105588    0.040055   -0.118289   -0.123525   -0.128323   -0.133265    0.124263   -0.304935   -0.308852   -0.314414   -0.309873   -0.105238   -0.042054    0.036398    0.018052   -0.006094   -0.093575   -0.013307   -0.001928   -0.057407   -0.304591   -0.287194   -0.286327   -0.284489    0.025270    0.047353    0.041526    0.010459   -0.173429   -0.176693   -0.181669   -0.178381   -0.306856   -0.313097   -0.311809   -0.310313   -0.229963   -0.233574   -0.244401   -0.242210   -0.158359   -0.161359   -0.165031   -0.162155   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402   -0.387137    2.248781   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  2.651885e+09  2.881755e+09   8.668186       8.668186\n",
      "  0.258006   0.146956   0.230236   0.256950  -0.145472  -0.148437  -0.154545  -0.149779  -0.270185  -0.211479   -0.294268   -0.134525   -0.276596   -0.263435   -0.264404   -0.259382   -0.253705   -0.253933   -0.271356   -0.263543   -0.268845   -0.275795   -0.289448   -0.290355   -0.135124   -0.165186   -0.067262   -0.227600   -0.217896   -0.179999   -0.094521   -0.240507   -0.129178   -0.131957   -0.137849   -0.137518    0.124263   -0.328630   -0.330859   -0.338907   -0.321606   -0.368210   -0.316520   -0.242420   -0.282473   -0.290753   -0.330520   -0.326775   -0.340213   -0.335049   -0.355167   -0.329924   -0.332354   -0.334009   -0.333922   -0.304666   -0.307089   -0.298945   -0.209779   -0.215219   -0.217270   -0.212485   -0.328616   -0.333683   -0.333962   -0.322663   -0.363767   -0.373084   -0.374244   -0.385136   -0.178814   -0.183220   -0.184798   -0.178817   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  9.402700e+07  1.252006e+08  33.153923      33.153923\n",
      "  0.222727   0.127075   0.199931   0.230583  -0.090200  -0.097483  -0.101614  -0.101742  -0.487214  -0.050506   -0.111106   -0.127819    0.502063    0.385511    0.400076    0.378303    0.202063    0.181383    0.189922    0.193599   -0.005970   -0.015363   -0.014611   -0.003457    0.717128    0.687948    0.445711    0.367290   -0.090189   -0.092980   -0.024041   -0.234065   -0.022917   -0.041336   -0.041061   -0.035669    0.124263    0.006299   -0.022504   -0.018962   -0.020084   -0.219439    0.049772    0.004657   -0.077777   -0.126759    0.037176    0.027875   -0.001933   -0.086300    0.238720    0.153283    0.136338    0.170162    0.487885    0.347230    0.351659    0.330186   -0.089130   -0.095814   -0.097449   -0.093920   -0.014067   -0.042686   -0.039345   -0.036892   -0.067262   -0.075080   -0.081722   -0.081098   -0.088895   -0.095251   -0.096099   -0.092269   -0.209379   -0.360714   -0.236242   -0.224031   -0.464402   -0.387137    2.248781   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  5.899600e+09  5.584011e+09  -5.349322       5.349322\n",
      "  0.253827   0.144927   0.229040   0.257491  -0.085852  -0.016698  -0.078722  -0.075940   0.376519   0.307806    0.142313    0.023288    0.180179    0.237643    0.118464    0.123984   -0.226908   -0.224748   -0.240834   -0.218224   -0.267795   -0.274802   -0.288366   -0.288790    0.231935    1.024839    0.347704   -0.422938    0.058811    0.442662    0.097384   -0.379178   -0.115988   -0.120217   -0.124845   -0.128435    0.124263   -0.278749   -0.282984   -0.293283   -0.290518    0.015493    0.179327    0.777884    0.248008   -0.421235   -0.007992   -0.032898   -0.009992   -0.000136   -0.349197   -0.326628   -0.326320   -0.327867    0.041244    0.085887   -0.009254   -0.001534    0.071733    0.078159    0.077739    0.076918   -0.289137   -0.295554   -0.295493   -0.295156    0.415183    0.443865    0.450870    0.429267    0.017997    0.020174    0.017494    0.021708   -0.209379   -0.360714   -0.236242   -0.224031    2.153308   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473    1.137550   -1.137550  3.248000e+09  3.065888e+09  -5.606904       5.606904\n",
      "  0.083761   0.060746   0.004200   0.124343  -0.107678  -0.119340  -0.104553  -0.100100  -0.078153  -0.040225   -0.027604   -0.140840    0.093766    0.037704    0.036598    0.028005    0.033459    0.010663    0.016407    0.037109   -0.062972   -0.092294   -0.096593   -0.079093    0.333192    0.382565    0.122496    0.086123   -0.064700   -0.072777   -0.048760   -0.167236   -0.117525   -0.122148   -0.128448   -0.136788    0.124263   -0.223598   -0.228656   -0.236079   -0.234018   -0.279623   -0.071352   -0.048465   -0.175947   -0.191476   -0.106599   -0.119580   -0.226704   -0.263109   -0.009013   -0.065174   -0.065677   -0.056023    0.080390    0.013485    0.002835   -0.006334   -0.104069   -0.111517   -0.116102   -0.110458   -0.181648   -0.204579   -0.202023   -0.195196    0.103249    0.091775    0.072150    0.085254   -0.130952   -0.137807   -0.140499   -0.135324   -0.209379    2.772280   -0.236242   -0.224031   -0.464402   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  2.802100e+09  2.882588e+09   2.872406       2.872406\n",
      "  0.107130   0.106920   0.197174   0.174134  -0.128846  -0.146173  -0.149514  -0.149350  -0.132145  -0.155728   -0.224937   -0.129043   -0.283902   -0.269216   -0.274560   -0.267483   -0.270027   -0.288467   -0.301291   -0.293419   -0.232751   -0.249985   -0.260268   -0.259634    0.122239    0.268652   -0.081859   -0.340067   -0.106510   -0.067637   -0.106015   -0.238124   -0.088126   -0.091581   -0.081946   -0.101723    0.124263   -0.187702   -0.196185   -0.200538   -0.201174   -0.367240   -0.178816   -0.101617   -0.264740   -0.316858   -0.171356   -0.133041   -0.297963   -0.358915   -0.155420   -0.162207   -0.181223   -0.193113   -0.270098   -0.248303   -0.271693   -0.273703   -0.190022   -0.197436   -0.199490   -0.195542   -0.206237   -0.216276   -0.216147   -0.217624   -0.384780   -0.400200   -0.403969   -0.404684   -0.153708   -0.159367   -0.160417   -0.157099   -0.209379    2.772280   -0.236242   -0.224031   -0.464402   -0.387137   -0.444685   -0.388373   -0.222243   -0.259294   -0.170473   -0.879082    0.879082  4.902120e+08  4.396134e+08 -10.321772      10.321772\n",
      "\n",
      "MAPE over the whole test set: 260.8693%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 1. Collect Predictions from Test Set\n",
    "# ============================================================\n",
    "y_true_all, y_pred_all, X_all = [], [], []\n",
    "\n",
    "for xb, yb in test_ds:\n",
    "    preds = model.predict(xb, verbose=0)\n",
    "    X_all.append(xb.numpy())\n",
    "    y_true_all.append(yb.numpy().reshape(-1))\n",
    "    y_pred_all.append(preds.reshape(-1))\n",
    "\n",
    "# Concatenate results\n",
    "X_all      = np.vstack(X_all)\n",
    "y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "y_pred_all = np.concatenate(y_pred_all, axis=0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Compute Percentage Errors\n",
    "# ============================================================\n",
    "denom          = np.maximum(np.abs(y_true_all), 1e-12)   # avoid division by zero\n",
    "pct_error      = (y_pred_all - y_true_all) / denom * 100.0\n",
    "abs_pct_error  = np.abs(pct_error)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Build Results DataFrame\n",
    "# ============================================================\n",
    "results_df = pd.DataFrame(\n",
    "    np.hstack([\n",
    "        X_all,\n",
    "        y_true_all[:, None],\n",
    "        y_pred_all[:, None],\n",
    "        pct_error[:, None],\n",
    "        abs_pct_error[:, None]\n",
    "    ]),\n",
    "    columns=[f\"feature_{i}\" for i in range(X_all.shape[1])]\n",
    "           + [\"y_true\", \"y_pred\", \"pct_error\", \"abs_pct_error\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Display Sample Predictions\n",
    "# ============================================================\n",
    "print(\"\\n=== Sample Predictions (25 rows) ===\")\n",
    "print(results_df.head(25).to_string(index=False))\n",
    "# Alternative (random sample):\n",
    "# print(results_df.sample(25, random_state=42).to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Aggregate Statistics\n",
    "# ============================================================\n",
    "mape = abs_pct_error.mean()\n",
    "print(f\"\\nMAPE over the whole test set: {mape:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1760403611580,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "_13_H05NsbPq",
    "outputId": "ca299498-3848-40aa-df9c-98e13af44bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full-Test Evaluation ===\n",
      "MAE mean   : 362688384.000000\n",
      "MAE median : 47272464.000000\n",
      "MAE std    (per sample): 1352621440.000000\n",
      "MAE std    (per batch) : 112655840.000000\n",
      "RMSE mean    : 1400402688.000000\n",
      "RMSE std   (per batch) : 521050784.000000\n",
      "RÂ² mean       : 0.972895\n",
      "RÂ² std     (per batch) : 0.008689\n",
      "SuccessRate@10%: 56.73%\n",
      "SuccessRate@20%: 78.36%\n",
      "SuccessRate@40%: 89.47%\n",
      "\n",
      "--- Baseline (predict train mean) ---\n",
      "Train mean      : 2673319936.000000\n",
      "Baseline MAE    : 3391386112.000000\n",
      "Baseline RMSE   : 8507461120.000000\n",
      "Baseline R^2    : -0.000329\n",
      "Baseline SR@10% : 3.80%\n",
      "Baseline SR@20% : 6.14%\n",
      "Baseline SR@40% : 14.04%\n",
      "\n",
      "--- Target scale (test set) ---\n",
      "y_true mean : 2827531264.000000\n",
      "y_true std  : 8506063360.000000\n",
      "y_true min  : -72319000.000000\n",
      "y_true max  : 97826996224.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------- Collect predictions on the entire test set ----------\n",
    "y_true_all, y_pred_all = [], []\n",
    "for xb, yb in test_ds:\n",
    "    preds = model.predict(xb, verbose=0)\n",
    "    y_true_all.append(yb.numpy().reshape(-1))\n",
    "    y_pred_all.append(preds.reshape(-1))\n",
    "\n",
    "y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "y_pred_all = np.concatenate(y_pred_all, axis=0)\n",
    "\n",
    "# ---------- Per-sample errors ----------\n",
    "eps        = 1e-12\n",
    "errors     = y_pred_all - y_true_all\n",
    "abs_errors = np.abs(errors)\n",
    "sq_errors  = errors**2\n",
    "\n",
    "# Success rates\n",
    "pct_error     = (y_pred_all - y_true_all) / np.maximum(np.abs(y_true_all), eps) * 100.0\n",
    "abs_pct_error = np.abs(pct_error)\n",
    "success_rate_10 = np.mean(abs_pct_error < 10.0) * 100.0\n",
    "success_rate_20 = np.mean(abs_pct_error < 20.0) * 100.0\n",
    "success_rate_40 = np.mean(abs_pct_error < 40.0) * 100.0\n",
    "\n",
    "# ---------- Global aggregate metrics (whole test set) ----------\n",
    "model_mae_mean   = np.mean(abs_errors)\n",
    "model_mae_median = np.median(abs_errors)\n",
    "model_mae_std    = np.std(abs_errors)\n",
    "\n",
    "model_rmse_mean  = np.sqrt(np.mean(sq_errors))   # proper RMSE on full dataset\n",
    "model_r2_mean    = 1.0 - np.sum(sq_errors) / np.sum((y_true_all - np.mean(y_true_all))**2)\n",
    "\n",
    "# ---------- Batch-level metrics ----------\n",
    "mae_per_batch, rmse_per_batch, r2_per_batch = [], [], []\n",
    "for xb, yb in test_ds:\n",
    "    preds = model.predict(xb, verbose=0).reshape(-1)\n",
    "    yb_np = yb.numpy().reshape(-1)\n",
    "    errors = preds - yb_np\n",
    "    abs_errors_b = np.abs(errors)\n",
    "    sq_errors_b  = errors**2\n",
    "\n",
    "    mae_batch  = np.mean(abs_errors_b)\n",
    "    rmse_batch = np.sqrt(np.mean(sq_errors_b))\n",
    "    r2_batch   = 1 - np.sum(sq_errors_b) / np.sum((yb_np - np.mean(yb_np))**2 + eps)\n",
    "\n",
    "    mae_per_batch.append(mae_batch)\n",
    "    rmse_per_batch.append(rmse_batch)\n",
    "    r2_per_batch.append(r2_batch)\n",
    "\n",
    "model_mae_std_batch  = np.std(mae_per_batch)\n",
    "model_rmse_std_batch = np.std(rmse_per_batch)\n",
    "model_r2_std_batch   = np.std(r2_per_batch)\n",
    "\n",
    "# ---------- Baseline (predict train mean) ----------\n",
    "def compute_train_mean_fallback():\n",
    "    if 'y_train' in globals() and y_train is not None:\n",
    "        return float(np.mean(y_train))\n",
    "    acc, n = 0.0, 0\n",
    "    for _, yb in train_ds:\n",
    "        yb_np = yb.numpy().reshape(-1)\n",
    "        acc += float(np.sum(yb_np))\n",
    "        n   += yb_np.size\n",
    "    return acc / max(n, 1)\n",
    "\n",
    "y_train_mean = compute_train_mean_fallback()\n",
    "baseline_preds = np.full_like(y_true_all, y_train_mean)\n",
    "\n",
    "baseline_abs_pct_error = np.abs((baseline_preds - y_true_all) /\n",
    "                                np.maximum(np.abs(y_true_all), eps)) * 100.0\n",
    "baseline_mae    = np.mean(np.abs(y_true_all - baseline_preds))\n",
    "baseline_rmse   = np.sqrt(np.mean((y_true_all - baseline_preds) ** 2))\n",
    "baseline_ss_res = np.sum((y_true_all - baseline_preds) ** 2)\n",
    "baseline_r2     = 1.0 - baseline_ss_res / np.sum((y_true_all - np.mean(y_true_all))**2)\n",
    "\n",
    "baseline_sr_10  = np.mean(baseline_abs_pct_error < 10.0) * 100.0\n",
    "baseline_sr_20  = np.mean(baseline_abs_pct_error < 20.0) * 100.0\n",
    "baseline_sr_40  = np.mean(baseline_abs_pct_error < 40.0) * 100.0\n",
    "\n",
    "# ---------- Print Summary ----------\n",
    "print(\"=== Full-Test Evaluation ===\")\n",
    "print(f\"MAE mean   : {model_mae_mean:.6f}\")\n",
    "print(f\"MAE median : {model_mae_median:.6f}\")\n",
    "print(f\"MAE std    (per sample): {model_mae_std:.6f}\")\n",
    "print(f\"MAE std    (per batch) : {model_mae_std_batch:.6f}\")\n",
    "\n",
    "print(f\"RMSE mean    : {model_rmse_mean:.6f}\")\n",
    "print(f\"RMSE std   (per batch) : {model_rmse_std_batch:.6f}\")\n",
    "\n",
    "print(f\"RÂ² mean       : {model_r2_mean:.6f}\")\n",
    "print(f\"RÂ² std     (per batch) : {model_r2_std_batch:.6f}\")\n",
    "\n",
    "print(f\"SuccessRate@10%: {success_rate_10:.2f}%\")\n",
    "print(f\"SuccessRate@20%: {success_rate_20:.2f}%\")\n",
    "print(f\"SuccessRate@40%: {success_rate_40:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Baseline (predict train mean) ---\")\n",
    "print(f\"Train mean      : {y_train_mean:.6f}\")\n",
    "print(f\"Baseline MAE    : {baseline_mae:.6f}\")\n",
    "print(f\"Baseline RMSE   : {baseline_rmse:.6f}\")\n",
    "print(f\"Baseline R^2    : {baseline_r2:.6f}\")\n",
    "print(f\"Baseline SR@10% : {baseline_sr_10:.2f}%\")\n",
    "print(f\"Baseline SR@20% : {baseline_sr_20:.2f}%\")\n",
    "print(f\"Baseline SR@40% : {baseline_sr_40:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Target scale (test set) ---\")\n",
    "print(f\"y_true mean : {np.mean(y_true_all):.6f}\")\n",
    "print(f\"y_true std  : {np.std(y_true_all):.6f}\")\n",
    "print(f\"y_true min  : {np.min(y_true_all):.6f}\")\n",
    "print(f\"y_true max  : {np.max(y_true_all):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1760403611585,
     "user": {
      "displayName": "Yueyao Ren",
      "userId": "10057464056443293539"
     },
     "user_tz": 420
    },
    "id": "yZ-KRSgbxUgQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0iNxqKmhvE1cE+q5Li475",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
